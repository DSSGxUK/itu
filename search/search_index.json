{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mapping the World's Offline Population This project develops a predictive model to estimate the number of people connected to the internet (internet, in this instance, is a proxy for access to social and economic opportunities). The model was trained on a combination of survey data and multiple open source datasets. With the help of the custom engineered data gathering pipeline, the open source data can be collected for any given country and either the existing model can be applied or a new model can be trained if survey data is available. This project was built from June - August 2021 and is a collaboration between the Data Science for Social Good Fellowship at the University of Warwick, UK and the International Telecommunications Union , a UN Agency. Click on the pages on the right to learn more about our project and find out how to replicate it for yourself. Need help getting started? If you are a policy maker, we recommend you start by reading our introduction and conclusion. If you are a technical person or otherwise seeking to replicate what we did for your own project, we recommend starting at the Data Gathering section. If you are curious about who we are or what datasets we've used, start at About the Team or Exploratory Data Analysis.","title":"Home"},{"location":"#mapping-the-worlds-offline-population","text":"This project develops a predictive model to estimate the number of people connected to the internet (internet, in this instance, is a proxy for access to social and economic opportunities). The model was trained on a combination of survey data and multiple open source datasets. With the help of the custom engineered data gathering pipeline, the open source data can be collected for any given country and either the existing model can be applied or a new model can be trained if survey data is available. This project was built from June - August 2021 and is a collaboration between the Data Science for Social Good Fellowship at the University of Warwick, UK and the International Telecommunications Union , a UN Agency. Click on the pages on the right to learn more about our project and find out how to replicate it for yourself. Need help getting started? If you are a policy maker, we recommend you start by reading our introduction and conclusion. If you are a technical person or otherwise seeking to replicate what we did for your own project, we recommend starting at the Data Gathering section. If you are curious about who we are or what datasets we've used, start at About the Team or Exploratory Data Analysis.","title":"Mapping the World's Offline Population"},{"location":"Meta_data/","text":"Meta Data Information Folder Structure Below is the folder structure for the Github Repository. All of the notebooks and scripts are organized within the below folder structure and data is inputted with relative file paths. Therefore, once you set your own working directory, everything within should run on its own. conf/ data/ geodata/ meta/ school_loc/ fb/ opencellid/ satellite/ speedtest/ survey/ Brazil/ Thailand/ Philippines/ training_sets/ Brazil/ Thailand/ Philippines/ worldpop/ model/ notebooks/ src/ scripts/ map_offline data_gathering init .py opendata.py opendata_utils.py opendata_scrap.py opendata_facebook.py opendata_satellite.py feature_engineering init .py data_pipeline.py configs.py country.py survey.py school.py main.py requirements.txt Configurations We use configs.py to store the variable configurations that should be changed according to use-case: WD - Working Directory, e.g. 'C:/Users/itu/DSSGx/' COUNTRY - Country name for current use-case, e.g. 'Thailand' COUNTRY_CODE - Country code for current use-case, e.g. 'tha' AVAILABLE_COUNTRIES - Countries for which survey data with an internet connectivity ground truth variable is available, e.g. list('bra', 'tha') FEATURES - List of predictive features for use-case, e.g. list('speedtest', 'opencell', 'facebook', 'population', 'satellite'). This exemplary list contains each of the five open data sources we have used and they must be sytactically entered as shown. SURVEY_AREAS - Survey dataset geometry join type, 'tiles' if survey will be joined to country administrative region, province, etc., 'enumeration' if survey will be joined to enumeration area geometries SATELLITE_COLLECTIONS - Dictionary that has satellite image collection identifier as a key and image collection band names for multi-band images, e.g. '{'MODIS/006/MOD13A2': ['NDVI']}' SATELITTE_START_YEAR - Start year of satellite imagery collection to be used SATELITTE_END_YEAR - End year of satellite imagery collection to be used SATELITTE_BUFFER - Buffer to define school area for which satellite imagery will be collected, in kilometers SATELITTE_MAX_CALL_SIZE - Google Earth Engine API max feature collection length, by default 5000 points GOOGLE_SERVICES_ACCOUNT - Google Services Account to call Google Earth Engine API GOOGLE_EARTH_ENGINE_API_JSON_KEY - JSON key file name that is located under satellite folder OPENCELLID_ACCESS_TOKEN - OpenCelliD Project API access token as string FACEBOOK_MARKETING_API_ACCESS_TOKEN - Facebook Marketing API access token as string FACEBOOK_AD_ACCOUNT_ID - Facebook Ad account id as string FACEBOOK_CALL_LIMIT - Facebook Ads Management API maximum calls within one hour, by default it is 300 + 40 * (Number of Active Ads) FACEBOOK_RADIUS - Radius to define school area for which Facebook API data will be collected, in kilometers FACEBOOK_SCHOOL_DATA_LEN - # of schools in the dataset to keep facebook data with school length in the name in case schools wanted to be divided into chunks and also approximate Facebook API completion time SPEEDTEST_TILE_TYPE - Service type for Ookla Open Speedtest Dataset can be 'fixed' or 'mobile' representing fixed or mobile network performance aggregates of tiles SPEEDTEST_TILE_YEAR - Speedtest data year, e.g. 2021 SPEEDTEST_TILE_QUARTER - Speedtest data quarter, e.g. 2 POPULATION_DATASET_YEAR - Population counts dataset year Data Dictionaries Data dictionaries should be created for each predictor and survey dataset. Dictionaries for the open-source data and Brazilian, Thai and Philippino survey data already exist and should be located in the data/meta/ folder. An exemplary data dictionary (for speedtest data) is shown below: Number Name Description Type Binary Role Use Comment 1 avg_d_kbps The average download speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 2 avg_u_kbps The average upload speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 3 avg_lat_ms The average latency of all tests performed in the tile, represented in milliseconds num N predictor N 4 tests The number of tests taken in the tile num N predictor N 5 devices The number of unique devices contributing tests in the tile num N predictor N","title":"Meta Data Information"},{"location":"Meta_data/#meta-data-information","text":"","title":"Meta Data Information"},{"location":"Meta_data/#folder-structure","text":"Below is the folder structure for the Github Repository. All of the notebooks and scripts are organized within the below folder structure and data is inputted with relative file paths. Therefore, once you set your own working directory, everything within should run on its own. conf/ data/ geodata/ meta/ school_loc/ fb/ opencellid/ satellite/ speedtest/ survey/ Brazil/ Thailand/ Philippines/ training_sets/ Brazil/ Thailand/ Philippines/ worldpop/ model/ notebooks/ src/ scripts/ map_offline data_gathering init .py opendata.py opendata_utils.py opendata_scrap.py opendata_facebook.py opendata_satellite.py feature_engineering init .py data_pipeline.py configs.py country.py survey.py school.py main.py requirements.txt","title":"Folder Structure"},{"location":"Meta_data/#configurations","text":"We use configs.py to store the variable configurations that should be changed according to use-case: WD - Working Directory, e.g. 'C:/Users/itu/DSSGx/' COUNTRY - Country name for current use-case, e.g. 'Thailand' COUNTRY_CODE - Country code for current use-case, e.g. 'tha' AVAILABLE_COUNTRIES - Countries for which survey data with an internet connectivity ground truth variable is available, e.g. list('bra', 'tha') FEATURES - List of predictive features for use-case, e.g. list('speedtest', 'opencell', 'facebook', 'population', 'satellite'). This exemplary list contains each of the five open data sources we have used and they must be sytactically entered as shown. SURVEY_AREAS - Survey dataset geometry join type, 'tiles' if survey will be joined to country administrative region, province, etc., 'enumeration' if survey will be joined to enumeration area geometries SATELLITE_COLLECTIONS - Dictionary that has satellite image collection identifier as a key and image collection band names for multi-band images, e.g. '{'MODIS/006/MOD13A2': ['NDVI']}' SATELITTE_START_YEAR - Start year of satellite imagery collection to be used SATELITTE_END_YEAR - End year of satellite imagery collection to be used SATELITTE_BUFFER - Buffer to define school area for which satellite imagery will be collected, in kilometers SATELITTE_MAX_CALL_SIZE - Google Earth Engine API max feature collection length, by default 5000 points GOOGLE_SERVICES_ACCOUNT - Google Services Account to call Google Earth Engine API GOOGLE_EARTH_ENGINE_API_JSON_KEY - JSON key file name that is located under satellite folder OPENCELLID_ACCESS_TOKEN - OpenCelliD Project API access token as string FACEBOOK_MARKETING_API_ACCESS_TOKEN - Facebook Marketing API access token as string FACEBOOK_AD_ACCOUNT_ID - Facebook Ad account id as string FACEBOOK_CALL_LIMIT - Facebook Ads Management API maximum calls within one hour, by default it is 300 + 40 * (Number of Active Ads) FACEBOOK_RADIUS - Radius to define school area for which Facebook API data will be collected, in kilometers FACEBOOK_SCHOOL_DATA_LEN - # of schools in the dataset to keep facebook data with school length in the name in case schools wanted to be divided into chunks and also approximate Facebook API completion time SPEEDTEST_TILE_TYPE - Service type for Ookla Open Speedtest Dataset can be 'fixed' or 'mobile' representing fixed or mobile network performance aggregates of tiles SPEEDTEST_TILE_YEAR - Speedtest data year, e.g. 2021 SPEEDTEST_TILE_QUARTER - Speedtest data quarter, e.g. 2 POPULATION_DATASET_YEAR - Population counts dataset year","title":"Configurations"},{"location":"Meta_data/#data-dictionaries","text":"Data dictionaries should be created for each predictor and survey dataset. Dictionaries for the open-source data and Brazilian, Thai and Philippino survey data already exist and should be located in the data/meta/ folder. An exemplary data dictionary (for speedtest data) is shown below: Number Name Description Type Binary Role Use Comment 1 avg_d_kbps The average download speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 2 avg_u_kbps The average upload speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 3 avg_lat_ms The average latency of all tests performed in the tile, represented in milliseconds num N predictor N 4 tests The number of tests taken in the tile num N predictor N 5 devices The number of unique devices contributing tests in the tile num N predictor N","title":"Data Dictionaries"},{"location":"about/","text":"Who are we? Tova Perlman Tova hails from Philadelphia, PA. She graduated in May 2021 with a degree in Urban Spatial Analytics from the University of Pennsylvania and is looking forward to pursuing a career in geospatial data science. The highlight of her summer was meeting new colleagues located internationally and learning how to use geemap API for python. Utku Can Ozturk Utku Can is from Izmir, Turkey and spent the last year getting a Masters in Data Science at LMU Munich. Next year he will be continuing at school starting his Masters thesis. The highlight of his summer was creating the data gathering pipeline and working with such incredible teammates and mentors. Jonathan Cook Jonathan is from a small village in Leicestershire, England. He graduated in May 2021 with First Class Honours in BSc Theoretical Physics from University College London (UCL). Starting from October 2021, he will be a DeepMind Scholar studying MSc Computational Statistics and Machine Learning, also at UCL. Jonathan has learned so much from the incredibly talented DSSGx UK cohort of fellows and mentors. He is also grateful for the opportunity to work on such a globally critical social issue. Jacob Beck Jacob recently graduated with a Master's degree in Sociology from the University of Mannheim, Germany. Starting in fall 2021 he will be a PhD candidate for the Department of Statistics at LMU Munich. He most enjoyed the collaboration of people from different academic backgrounds and getting insights to Machine Learning approaches. Project Mentor: Daniel Townsend Daniel is a Canadian living in Berlin and working as a project management consultant. His practice focuses on helping organizations to realize strategic and visionary outcomes through policy, developing data systems and organizational change management. Daniel has over 15 years of experience in management consulting and policy development. He has a MSc from Imperial College London and is pursuing a Ph.D. at the University of Toronto. His highlight this summer was working with all the fellows. Technical Mentor: Robert Hager Robert resides in Munich, Germany and is a mathematician who took a break from his current position as a Data Scientist, working in the insurance sector, to be a technical mentor for the ITU project this summer. He has enjoyed contributing his experience in developing and delivering data science products. Who is ITU? \"The International Telecommunication Union (ITU) is the specialized United Nations agency for information and communication technologies (ICTs), driving innovation in ICTs together with 193 Member States and a membership of nearly 900 companies, universities, and international and regional organizations. Established over 150 years ago in 1865, ITU is the intergovernmental body responsible for coordinating the shared global use of the radio spectrum, promoting international cooperation in assigning satellite orbits, improving communication infrastructure in the developing world, and establishing the worldwide standards that foster seamless interconnection of a vast range of communications systems. From broadband networks to cutting-edge wireless technologies, aeronautical and maritime navigation, radio astronomy, oceanographic and satellite-based earth monitoring as well as converging fixed-mobile phone, internet and broadcasting technologies, ITU is committed to connecting the world. For more information, visit www.itu.int. Through ITU\u2019s commitment to connecting the world, ITU brings to Giga a wealth of experience in ICT data and building safe and secure connectivity for all, and achieving a vision of an information society where technologies enable and accelerate social, economic, and environmentally sustainable development. ITU is able to further contribute through its research, ICT statistics, as well as its advocacy work promoting effective, safe and trusted policy, legal and regulatory environments which foster the development of telecommunication and ICT networks, and build trust and confidence in the use of ICTs.\" Who is UNICEF? \"UNICEF works in some of the world\u2019s toughest places, to reach the world\u2019s most disadvantaged children. To save their lives. To defend their rights. To help them fulfill their potential. Across 190 countries and territories, we work for every child, everywhere, every day, to build a better world for everyone. Through UNICEF\u2019s Project Connect \u2014 a mapping and connectivity monitoring platform \u2014 UNICEF is able to provide a better understanding of where connectivity needs are in real time. So far, 800,000 schools in 15 countries have already been mapped, and counting: with more connectivity data, we can identify gaps in connectivity, aggregate demand, and collaborate on financing models necessary to connect disconnected schools to the internet.\" What is Project Giga? \"Giga will bring the power of meaningful connectivity to fast track young people\u2019s access to educational resources and opportunities. Giga will ensure every child is equipped with the digital public goods they need, and empowered to shape the future they want. Giga also serves as a platform to create the infrastructure necessary to provide digital connectivity to an entire country, for every community, and for every citizen. It is about using schools to identify demand for connectivity, as well as using schools as an analogy for learning and connecting where the community can come together and support its next generation in a world where we are all increasingly digital, where the skills that are required are not formal ones, necessarily, and where learning happens continuously.\" Excerpts above from the Project Giga website","title":"About the Team"},{"location":"about/#who-are-we","text":"Tova Perlman Tova hails from Philadelphia, PA. She graduated in May 2021 with a degree in Urban Spatial Analytics from the University of Pennsylvania and is looking forward to pursuing a career in geospatial data science. The highlight of her summer was meeting new colleagues located internationally and learning how to use geemap API for python. Utku Can Ozturk Utku Can is from Izmir, Turkey and spent the last year getting a Masters in Data Science at LMU Munich. Next year he will be continuing at school starting his Masters thesis. The highlight of his summer was creating the data gathering pipeline and working with such incredible teammates and mentors. Jonathan Cook Jonathan is from a small village in Leicestershire, England. He graduated in May 2021 with First Class Honours in BSc Theoretical Physics from University College London (UCL). Starting from October 2021, he will be a DeepMind Scholar studying MSc Computational Statistics and Machine Learning, also at UCL. Jonathan has learned so much from the incredibly talented DSSGx UK cohort of fellows and mentors. He is also grateful for the opportunity to work on such a globally critical social issue. Jacob Beck Jacob recently graduated with a Master's degree in Sociology from the University of Mannheim, Germany. Starting in fall 2021 he will be a PhD candidate for the Department of Statistics at LMU Munich. He most enjoyed the collaboration of people from different academic backgrounds and getting insights to Machine Learning approaches. Project Mentor: Daniel Townsend Daniel is a Canadian living in Berlin and working as a project management consultant. His practice focuses on helping organizations to realize strategic and visionary outcomes through policy, developing data systems and organizational change management. Daniel has over 15 years of experience in management consulting and policy development. He has a MSc from Imperial College London and is pursuing a Ph.D. at the University of Toronto. His highlight this summer was working with all the fellows. Technical Mentor: Robert Hager Robert resides in Munich, Germany and is a mathematician who took a break from his current position as a Data Scientist, working in the insurance sector, to be a technical mentor for the ITU project this summer. He has enjoyed contributing his experience in developing and delivering data science products.","title":"Who are we?"},{"location":"about/#who-is-itu","text":"\"The International Telecommunication Union (ITU) is the specialized United Nations agency for information and communication technologies (ICTs), driving innovation in ICTs together with 193 Member States and a membership of nearly 900 companies, universities, and international and regional organizations. Established over 150 years ago in 1865, ITU is the intergovernmental body responsible for coordinating the shared global use of the radio spectrum, promoting international cooperation in assigning satellite orbits, improving communication infrastructure in the developing world, and establishing the worldwide standards that foster seamless interconnection of a vast range of communications systems. From broadband networks to cutting-edge wireless technologies, aeronautical and maritime navigation, radio astronomy, oceanographic and satellite-based earth monitoring as well as converging fixed-mobile phone, internet and broadcasting technologies, ITU is committed to connecting the world. For more information, visit www.itu.int. Through ITU\u2019s commitment to connecting the world, ITU brings to Giga a wealth of experience in ICT data and building safe and secure connectivity for all, and achieving a vision of an information society where technologies enable and accelerate social, economic, and environmentally sustainable development. ITU is able to further contribute through its research, ICT statistics, as well as its advocacy work promoting effective, safe and trusted policy, legal and regulatory environments which foster the development of telecommunication and ICT networks, and build trust and confidence in the use of ICTs.\"","title":"Who is ITU?"},{"location":"about/#who-is-unicef","text":"\"UNICEF works in some of the world\u2019s toughest places, to reach the world\u2019s most disadvantaged children. To save their lives. To defend their rights. To help them fulfill their potential. Across 190 countries and territories, we work for every child, everywhere, every day, to build a better world for everyone. Through UNICEF\u2019s Project Connect \u2014 a mapping and connectivity monitoring platform \u2014 UNICEF is able to provide a better understanding of where connectivity needs are in real time. So far, 800,000 schools in 15 countries have already been mapped, and counting: with more connectivity data, we can identify gaps in connectivity, aggregate demand, and collaborate on financing models necessary to connect disconnected schools to the internet.\"","title":"Who is UNICEF?"},{"location":"about/#what-is-project-giga","text":"\"Giga will bring the power of meaningful connectivity to fast track young people\u2019s access to educational resources and opportunities. Giga will ensure every child is equipped with the digital public goods they need, and empowered to shape the future they want. Giga also serves as a platform to create the infrastructure necessary to provide digital connectivity to an entire country, for every community, and for every citizen. It is about using schools to identify demand for connectivity, as well as using schools as an analogy for learning and connecting where the community can come together and support its next generation in a world where we are all increasingly digital, where the skills that are required are not formal ones, necessarily, and where learning happens continuously.\" Excerpts above from the Project Giga website","title":"What is Project Giga?"},{"location":"conc/","text":"Discussion Throughout the project, the team has gained a variety of insights that can be used in future research and for extending the completed model for the pilot countries. The model is intended as a generic tool and is replicable across countries, while allowing for further customization for every additional country. The tool developed is capable of providing multiple outcomes: \u2022 First of all, the size of offline populations is estimated around schools and at the community level, across the country. \u2022 Then, based on the model application, a rank of priority can be given to schools, which can serve to inform a national connectivity roadmap. The prioritization of schools to be connected can be sorted by the absolute number of the offline population that can potentially be served by a new connection, or by the relative share of online population. In addition, it is possible to introduce additional criteria or indicators to adapt the ranking depending on higher-level policy priorities, e.g., by excluding densely populated or metropolitan areas. This focus could be placed if schools in less populated or rural areas experience specific challenges and would benefit most from Internet connectivity. \u2022 Moreover, the level of geographic aggregation of connectivity ratios can be readily modified up to the federal state or to the country level. The scripts and relevant documentation developed for this project can inform national connectivity initiatives and support institutions throughout their implementation. Development partners and initiatives, such as Giga, can likewise leverage the tool to enhance resource allocation for connecting schools, accelerate connectivity and maximize social impact. The analysis can be done for a country, across multiple countries as well as on the province, region or city level. \u2022 Last but not least, to support the work of institutions and partners, the team has developed a straightforward tool for data gathering and Exploratory Data Analysis (EDA). Creating quick map graphics or correlation plots might already indicate the presence of clear trends and inform policy and implementation discussions. Limitations During the project, we have faced certain limitations, in particular in the area of model evaluation. The model performs accurately within the original pilot country, however its level of applicability to countries from other global regions with differences in economic and social indicators is yet to be determined. Future applications and extension of the models can help to further refine and enhance the project outcome. In the case of Thailand in particular, the team encountered additional challenges with data quality. It was not possible to separate the different sources of error within the dataset from each other. In this specific case, for instance, we could not find out to which degree the geographic aggregation of the target variable or the fact that we used Open Street Maps (OSM) schools impacted model performance and error. In addition to the limitations faced, there are more uncertainties that might have affected the findings of our work and its future extensions. Those deserve further scrutiny and discussion. \u2022 As explained above, we cannot evaluate yet how OSM school data performs compared to official school location data. If this data source is to be used as an alternative to UNICEF school data, its representativity should be carefully reviewed. \u2022 In the case of Brazil ground truth data on connectivity rates is provided by taking household-level data in to account. The fact, that our result is a connectivity rate on the individual level could potentially bias our results. It is unclear whether household and individual level ratios are interchangeable. \u2022 Furthermore, while Facebook data could contribute to producing reliable near real-time estimates of Internet population in some countries or regions, in some cases it could prove insufficiently representative. Since the popularity of Facebook varies over time, between countries or according to demographics, estimating Internet use solely based on it might lead to misinterpretation. For instance, if a large metropolitan area shows a low percentage of Facebook users, this is not yet an indication of low overall Internet penetration, but could be due to the fact that another social media platform (such as Instagram) has overtaken it in popularity. An analysis of the distribution of Facebook users among social groups could improve data interpretation. \u2022 Another potential source of bias could be the fact that, in our original approach, survey data is used as ground truth. Survey data are typically prone to bias due to, for instance, misreporting or systematic non-response (e.g., survey respondents have previously stated that they are not using the Internet but in the same survey declared themselves as Facebook users). Importantly, non-response could be problematic since the offline population is less likely to respond to the survey. A potential corroborating measure here could be comparing the distribution of demographic data within the survey (e.g., income, gender) with other open data sources. \u2022 Ultimately, if the analysis is extended to more countries, the availability of microdata could introduce bias in the selection of countries to be targeted. It would be fair to assume that countries where no microdata on telecommunications use exists are likely to display lower connectivity levels. Therefore, further research should attempt to provide sound estimates of the offline population in the absence of national microdata, i.e. a global model that can be reliably applied to most countries. Applying a robust model that can accommodate various countries\u2019 circumstances without requiring microdata would then also be fully open-source. Next Steps Additional Methods Multiple additional methods could be applied to our existing models and analyses. This can either lead to both, a further corroboration of results and the extension/improvement of the existing work. \u2022 Full scoring on Brazil Given our existing champion model, all schools outside the featured enumeration areas can now be scored with an estimated level of connectivity. This could be done for a sample of schools as a sanity check or for the entire country, which would yield a similar map to the Giga connectivity map. \u2022 Statistical robustness checks In particular, two steps could be reasonable additions that take the geographic nature of the data even more into account: Estimating a Geographically Weighted Regression (GWR) Performing spatial cross-validation of the models \u2022 Experiment with featured data The way some of the model features are configured could be further fine-tuned. First, it is worth experimenting with Facebook data to explore whether it can be used as a standalone proxy variable for online population. This would provide a quick and efficient tool to estimate connectivity and allow for the extension of the project to a greater number of countries. Separately, pulling OSM school locations for Brazil and applying the model to this new sample of schools could yield insights on the representativity of OSM data. Finally, buffer zones around schools are currently constant in diameter, but could be varied according to specific variables (e.g., population data). Extension to other countries The existing model can be applied to every country with OSM school location data available. However, model evaluation or model training requires two more components: \u2022 Microdata (on household, individual or enumeration area level) \u2022 The respective shapefiles/geolocation of the enumeration area Our current experience indicates that analyzing larger geographical aggregates such as provinces diminishes model performance and interpretability. An enumeration area is assumed to be a homogeneous area, whereas the variation of localized connectivity levels within a federal state for instance is expected to be much higher. Further model training With more countries being included in the connectivity analysis, more options of model combinations open up. As long as the requirements (microdata and shapefiles) are met, an individual model can and should be trained for each country and evaluated separately. In addition, joint models with multiple countries could result in a high predictive power on a global level and increase model robustness. Comparing the performance of combined and single-country models can give insights on the impact of country specific differences. Furthermore, the idea of training regional models (e.g., a model for South-East Asian countries) appears worth exploring. National stakeholders that are not able to share enumeration area level microdata (e.g., for legal reasons due to anonymity restrictions) can obtain the scripts and additional content developed through this project and run a custom model training locally. Additional data sources Introducing additional data sources in the modelling can improve its predictive power and replicability. \u2022 Country-level data When extending the models to multiple countries, including country level variables might allow for boosting the original model. Possible additions could be urbanization rate, GDP, public spending on telecommunications or other indices of human/infrastructural development. In doing so, country-specific intercepts are featured in the estimation and will assist accounting for differences in overall connectivity between countries. \u2022 Additional content from survey/school data The original models have so far only been using the necessarily required features of survey and school location data. To keep our approach as generic and replicable to other countries as possible, additional information from these data sources has not been built in the models. However, in order to enhance national models, additional data series from surveys can be usefully integrated (e.g., connectivity-related variables, reasons for (non-)connectivity, demographic information such as age or gender or information on household size or income). Similarly, additional information contained in school location datasets such as pupil count or computer availability could also be used in further analysis (e.g., for estimating the propensity or ease of connection for a specific school). \u2022 Further data Arguably, existing models could be improved by adding more features to the training datasets. Possible additions could stem from official data provided by national stakeholders such as Ministries, regulatory authorities, telecom operators and service providers (e.g., coverage and characteristics of existing infrastructure), or digital platforms and monitoring services (such as Google, Facebook or Cisco Analytics). As a minimum requirement, a hypothetical logical connection between feature and target variable should always exist.","title":"Discussion"},{"location":"conc/#discussion","text":"Throughout the project, the team has gained a variety of insights that can be used in future research and for extending the completed model for the pilot countries. The model is intended as a generic tool and is replicable across countries, while allowing for further customization for every additional country. The tool developed is capable of providing multiple outcomes: \u2022 First of all, the size of offline populations is estimated around schools and at the community level, across the country. \u2022 Then, based on the model application, a rank of priority can be given to schools, which can serve to inform a national connectivity roadmap. The prioritization of schools to be connected can be sorted by the absolute number of the offline population that can potentially be served by a new connection, or by the relative share of online population. In addition, it is possible to introduce additional criteria or indicators to adapt the ranking depending on higher-level policy priorities, e.g., by excluding densely populated or metropolitan areas. This focus could be placed if schools in less populated or rural areas experience specific challenges and would benefit most from Internet connectivity. \u2022 Moreover, the level of geographic aggregation of connectivity ratios can be readily modified up to the federal state or to the country level. The scripts and relevant documentation developed for this project can inform national connectivity initiatives and support institutions throughout their implementation. Development partners and initiatives, such as Giga, can likewise leverage the tool to enhance resource allocation for connecting schools, accelerate connectivity and maximize social impact. The analysis can be done for a country, across multiple countries as well as on the province, region or city level. \u2022 Last but not least, to support the work of institutions and partners, the team has developed a straightforward tool for data gathering and Exploratory Data Analysis (EDA). Creating quick map graphics or correlation plots might already indicate the presence of clear trends and inform policy and implementation discussions.","title":"Discussion"},{"location":"conc/#limitations","text":"During the project, we have faced certain limitations, in particular in the area of model evaluation. The model performs accurately within the original pilot country, however its level of applicability to countries from other global regions with differences in economic and social indicators is yet to be determined. Future applications and extension of the models can help to further refine and enhance the project outcome. In the case of Thailand in particular, the team encountered additional challenges with data quality. It was not possible to separate the different sources of error within the dataset from each other. In this specific case, for instance, we could not find out to which degree the geographic aggregation of the target variable or the fact that we used Open Street Maps (OSM) schools impacted model performance and error. In addition to the limitations faced, there are more uncertainties that might have affected the findings of our work and its future extensions. Those deserve further scrutiny and discussion. \u2022 As explained above, we cannot evaluate yet how OSM school data performs compared to official school location data. If this data source is to be used as an alternative to UNICEF school data, its representativity should be carefully reviewed. \u2022 In the case of Brazil ground truth data on connectivity rates is provided by taking household-level data in to account. The fact, that our result is a connectivity rate on the individual level could potentially bias our results. It is unclear whether household and individual level ratios are interchangeable. \u2022 Furthermore, while Facebook data could contribute to producing reliable near real-time estimates of Internet population in some countries or regions, in some cases it could prove insufficiently representative. Since the popularity of Facebook varies over time, between countries or according to demographics, estimating Internet use solely based on it might lead to misinterpretation. For instance, if a large metropolitan area shows a low percentage of Facebook users, this is not yet an indication of low overall Internet penetration, but could be due to the fact that another social media platform (such as Instagram) has overtaken it in popularity. An analysis of the distribution of Facebook users among social groups could improve data interpretation. \u2022 Another potential source of bias could be the fact that, in our original approach, survey data is used as ground truth. Survey data are typically prone to bias due to, for instance, misreporting or systematic non-response (e.g., survey respondents have previously stated that they are not using the Internet but in the same survey declared themselves as Facebook users). Importantly, non-response could be problematic since the offline population is less likely to respond to the survey. A potential corroborating measure here could be comparing the distribution of demographic data within the survey (e.g., income, gender) with other open data sources. \u2022 Ultimately, if the analysis is extended to more countries, the availability of microdata could introduce bias in the selection of countries to be targeted. It would be fair to assume that countries where no microdata on telecommunications use exists are likely to display lower connectivity levels. Therefore, further research should attempt to provide sound estimates of the offline population in the absence of national microdata, i.e. a global model that can be reliably applied to most countries. Applying a robust model that can accommodate various countries\u2019 circumstances without requiring microdata would then also be fully open-source.","title":"Limitations"},{"location":"conc/#next-steps","text":"","title":"Next Steps"},{"location":"conc/#additional-methods","text":"Multiple additional methods could be applied to our existing models and analyses. This can either lead to both, a further corroboration of results and the extension/improvement of the existing work. \u2022 Full scoring on Brazil Given our existing champion model, all schools outside the featured enumeration areas can now be scored with an estimated level of connectivity. This could be done for a sample of schools as a sanity check or for the entire country, which would yield a similar map to the Giga connectivity map. \u2022 Statistical robustness checks In particular, two steps could be reasonable additions that take the geographic nature of the data even more into account: Estimating a Geographically Weighted Regression (GWR) Performing spatial cross-validation of the models \u2022 Experiment with featured data The way some of the model features are configured could be further fine-tuned. First, it is worth experimenting with Facebook data to explore whether it can be used as a standalone proxy variable for online population. This would provide a quick and efficient tool to estimate connectivity and allow for the extension of the project to a greater number of countries. Separately, pulling OSM school locations for Brazil and applying the model to this new sample of schools could yield insights on the representativity of OSM data. Finally, buffer zones around schools are currently constant in diameter, but could be varied according to specific variables (e.g., population data).","title":"Additional Methods"},{"location":"conc/#extension-to-other-countries","text":"The existing model can be applied to every country with OSM school location data available. However, model evaluation or model training requires two more components: \u2022 Microdata (on household, individual or enumeration area level) \u2022 The respective shapefiles/geolocation of the enumeration area Our current experience indicates that analyzing larger geographical aggregates such as provinces diminishes model performance and interpretability. An enumeration area is assumed to be a homogeneous area, whereas the variation of localized connectivity levels within a federal state for instance is expected to be much higher. Further model training With more countries being included in the connectivity analysis, more options of model combinations open up. As long as the requirements (microdata and shapefiles) are met, an individual model can and should be trained for each country and evaluated separately. In addition, joint models with multiple countries could result in a high predictive power on a global level and increase model robustness. Comparing the performance of combined and single-country models can give insights on the impact of country specific differences. Furthermore, the idea of training regional models (e.g., a model for South-East Asian countries) appears worth exploring. National stakeholders that are not able to share enumeration area level microdata (e.g., for legal reasons due to anonymity restrictions) can obtain the scripts and additional content developed through this project and run a custom model training locally.","title":"Extension to other countries"},{"location":"conc/#additional-data-sources","text":"Introducing additional data sources in the modelling can improve its predictive power and replicability. \u2022 Country-level data When extending the models to multiple countries, including country level variables might allow for boosting the original model. Possible additions could be urbanization rate, GDP, public spending on telecommunications or other indices of human/infrastructural development. In doing so, country-specific intercepts are featured in the estimation and will assist accounting for differences in overall connectivity between countries. \u2022 Additional content from survey/school data The original models have so far only been using the necessarily required features of survey and school location data. To keep our approach as generic and replicable to other countries as possible, additional information from these data sources has not been built in the models. However, in order to enhance national models, additional data series from surveys can be usefully integrated (e.g., connectivity-related variables, reasons for (non-)connectivity, demographic information such as age or gender or information on household size or income). Similarly, additional information contained in school location datasets such as pupil count or computer availability could also be used in further analysis (e.g., for estimating the propensity or ease of connection for a specific school). \u2022 Further data Arguably, existing models could be improved by adding more features to the training datasets. Possible additions could stem from official data provided by national stakeholders such as Ministries, regulatory authorities, telecom operators and service providers (e.g., coverage and characteristics of existing infrastructure), or digital platforms and monitoring services (such as Google, Facebook or Cisco Analytics). As a minimum requirement, a hypothetical logical connection between feature and target variable should always exist.","title":"Additional data sources"},{"location":"configs/","text":"Configurations We use configs.py to store the variable configurations that should be changed according to use-case: WD - Working Directory, e.g. 'C:/Users/itu/DSSGx/' COUNTRY - Country name for current use-case, e.g. 'Thailand' COUNTRY_CODE - Country code for current use-case, e.g. 'tha' AVAILABLE_COUNTRIES - Countries for which survey data with an internet connectivity ground truth variable is available, e.g. list('bra', 'tha') FEATURES - List of predictive features for use-case, e.g. list('speedtest', 'opencell', 'facebook', 'population', 'satellite'). This exemplary list contains each of the five open data sources we have used and they must be sytactically entered as shown. SURVEY_AREAS - Survey dataset geometry join type, 'tiles' if survey will be joined to country administrative region, province, etc., 'enumeration' if survey will be joined to enumeration area geometries SATELLITE_COLLECTIONS - Dictionary that has satellite image collection identifier as a key and image collection band names for multi-band images, e.g. '{'MODIS/006/MOD13A2': ['NDVI']}' SATELITTE_START_YEAR - Start year of satellite imagery collection to be used SATELITTE_END_YEAR - End year of satellite imagery collection to be used SATELITTE_BUFFER - Buffer to define school area for which satellite imagery will be collected, in kilometers SATELITTE_MAX_CALL_SIZE - Google Earth Engine API max feature collection length, by default 5000 points GOOGLE_SERVICES_ACCOUNT - Google Services Account to call Google Earth Engine API GOOGLE_EARTH_ENGINE_API_JSON_KEY - JSON key file name that is located under satellite folder OPENCELLID_ACCESS_TOKEN - OpenCelliD Project API access token as string FACEBOOK_MARKETING_API_ACCESS_TOKEN - Facebook Marketing API access token as string FACEBOOK_AD_ACCOUNT_ID - Facebook Ad account id as string FACEBOOK_CALL_LIMIT - Facebook Ads Management API maximum calls within one hour, by default it is 300 + 40 * (Number of Active Ads) FACEBOOK_RADIUS - Radius to define school area for which Facebook API data will be collected, in kilometers FACEBOOK_SCHOOL_DATA_LEN - # of schools in the dataset to keep facebook data with school length in the name in case schools wanted to be divided into chunks and also approximate Facebook API completion time SPEEDTEST_TILE_TYPE - Service type for Ookla Open Speedtest Dataset can be 'fixed' or 'mobile' representing fixed or mobile network performance aggregates of tiles SPEEDTEST_TILE_YEAR - Speedtest data year, e.g. 2021 SPEEDTEST_TILE_QUARTER - Speedtest data quarter, e.g. 2 POPULATION_DATASET_YEAR - Population counts dataset year","title":"Configurations"},{"location":"configs/#configurations","text":"We use configs.py to store the variable configurations that should be changed according to use-case: WD - Working Directory, e.g. 'C:/Users/itu/DSSGx/' COUNTRY - Country name for current use-case, e.g. 'Thailand' COUNTRY_CODE - Country code for current use-case, e.g. 'tha' AVAILABLE_COUNTRIES - Countries for which survey data with an internet connectivity ground truth variable is available, e.g. list('bra', 'tha') FEATURES - List of predictive features for use-case, e.g. list('speedtest', 'opencell', 'facebook', 'population', 'satellite'). This exemplary list contains each of the five open data sources we have used and they must be sytactically entered as shown. SURVEY_AREAS - Survey dataset geometry join type, 'tiles' if survey will be joined to country administrative region, province, etc., 'enumeration' if survey will be joined to enumeration area geometries SATELLITE_COLLECTIONS - Dictionary that has satellite image collection identifier as a key and image collection band names for multi-band images, e.g. '{'MODIS/006/MOD13A2': ['NDVI']}' SATELITTE_START_YEAR - Start year of satellite imagery collection to be used SATELITTE_END_YEAR - End year of satellite imagery collection to be used SATELITTE_BUFFER - Buffer to define school area for which satellite imagery will be collected, in kilometers SATELITTE_MAX_CALL_SIZE - Google Earth Engine API max feature collection length, by default 5000 points GOOGLE_SERVICES_ACCOUNT - Google Services Account to call Google Earth Engine API GOOGLE_EARTH_ENGINE_API_JSON_KEY - JSON key file name that is located under satellite folder OPENCELLID_ACCESS_TOKEN - OpenCelliD Project API access token as string FACEBOOK_MARKETING_API_ACCESS_TOKEN - Facebook Marketing API access token as string FACEBOOK_AD_ACCOUNT_ID - Facebook Ad account id as string FACEBOOK_CALL_LIMIT - Facebook Ads Management API maximum calls within one hour, by default it is 300 + 40 * (Number of Active Ads) FACEBOOK_RADIUS - Radius to define school area for which Facebook API data will be collected, in kilometers FACEBOOK_SCHOOL_DATA_LEN - # of schools in the dataset to keep facebook data with school length in the name in case schools wanted to be divided into chunks and also approximate Facebook API completion time SPEEDTEST_TILE_TYPE - Service type for Ookla Open Speedtest Dataset can be 'fixed' or 'mobile' representing fixed or mobile network performance aggregates of tiles SPEEDTEST_TILE_YEAR - Speedtest data year, e.g. 2021 SPEEDTEST_TILE_QUARTER - Speedtest data quarter, e.g. 2 POPULATION_DATASET_YEAR - Population counts dataset year","title":"Configurations"},{"location":"data_dictionaries/","text":"Data Dictionaries Data dictionaries should be created for each predictor and survey dataset. Dictionaries for the open-source data and Brazilian, Thai and Philippino survey data already exist and should be located in the data/meta/ folder. An exemplary data dictionary (for speedtest data) is shown below: Number Name Description Type Binary Role Use Comment 1 avg_d_kbps The average download speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 2 avg_u_kbps The average upload speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 3 avg_lat_ms The average latency of all tests performed in the tile, represented in milliseconds num N predictor N 4 tests The number of tests taken in the tile num N predictor N 5 devices The number of unique devices contributing tests in the tile num N predictor N","title":"Data Dictionaries"},{"location":"data_dictionaries/#data-dictionaries","text":"Data dictionaries should be created for each predictor and survey dataset. Dictionaries for the open-source data and Brazilian, Thai and Philippino survey data already exist and should be located in the data/meta/ folder. An exemplary data dictionary (for speedtest data) is shown below: Number Name Description Type Binary Role Use Comment 1 avg_d_kbps The average download speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 2 avg_u_kbps The average upload speed of all tests performed in the tile, represented in kilobits per second num N predictor Y mbps can also be used 3 avg_lat_ms The average latency of all tests performed in the tile, represented in milliseconds num N predictor N 4 tests The number of tests taken in the tile num N predictor N 5 devices The number of unique devices contributing tests in the tile num N predictor N","title":"Data Dictionaries"},{"location":"data_processing/","text":"Instructions to Create Model-Ready Dataset Create an environment by following these directions. It should have all dependencies you need installed. A note that if you have a mac, some of the packages like GDAL might be harder to install. Make sure you have the miniconda shell or another type of terminal installed. Type conda info --envs to see the other environments that exist. Then type conda env create tovaperlman/itu_test_02 . It should take a few moments to minutes for all the packages to download. Once this is done, you can check that it's an environment present in your computer by typing again conda info --envs Then to activate the environment you can type conda activate itu_test_02 Once you've activated the environment which has all the necessary packages and dependencies installed, navigate in the same terminal to wherever you've saved the repository. Within that navigate to src/scripts/map_offline/. Then open the repository within a code editor (Visual Studio Code, Atom, Pycharm, etc.) To generate a dataframe comprising any target variables and predictors that you wish to use, first set up the use case configurations in feature_engineering/configs.py. Details of the configurable variables and their expected assignments can be found in the 'configurations' section of the documentation. For this, you must separately get access tokens for open cell ID, Google Earth Engine, and Facebook API. Once you have these, log all of them in the configs file. For Open Cell ID, you just need to sign up for an account and it will give you an API Access Token. For Google Earth Engine, you must first sign up for a [Google Earth Engine account] (https://earthengine.google.com/). You cannot use Google Earth Engine unless your application has been approved. Once you receive the application approval email, you can log in to the Earth Engine Code Editor to get familiar with the JavaScript API. Having correctly set the desired configurations, all you need to do is run 'main.py'. Following this, a dataset for model training and/or application will be saved within a training_sets folder, which is situated within the data directory.","title":"Instructions to Create Model-Ready Dataset"},{"location":"data_processing/#instructions-to-create-model-ready-dataset","text":"Create an environment by following these directions. It should have all dependencies you need installed. A note that if you have a mac, some of the packages like GDAL might be harder to install. Make sure you have the miniconda shell or another type of terminal installed. Type conda info --envs to see the other environments that exist. Then type conda env create tovaperlman/itu_test_02 . It should take a few moments to minutes for all the packages to download. Once this is done, you can check that it's an environment present in your computer by typing again conda info --envs Then to activate the environment you can type conda activate itu_test_02 Once you've activated the environment which has all the necessary packages and dependencies installed, navigate in the same terminal to wherever you've saved the repository. Within that navigate to src/scripts/map_offline/. Then open the repository within a code editor (Visual Studio Code, Atom, Pycharm, etc.) To generate a dataframe comprising any target variables and predictors that you wish to use, first set up the use case configurations in feature_engineering/configs.py. Details of the configurable variables and their expected assignments can be found in the 'configurations' section of the documentation. For this, you must separately get access tokens for open cell ID, Google Earth Engine, and Facebook API. Once you have these, log all of them in the configs file. For Open Cell ID, you just need to sign up for an account and it will give you an API Access Token. For Google Earth Engine, you must first sign up for a [Google Earth Engine account] (https://earthengine.google.com/). You cannot use Google Earth Engine unless your application has been approved. Once you receive the application approval email, you can log in to the Earth Engine Code Editor to get familiar with the JavaScript API. Having correctly set the desired configurations, all you need to do is run 'main.py'. Following this, a dataset for model training and/or application will be saved within a training_sets folder, which is situated within the data directory.","title":"Instructions to Create Model-Ready Dataset"},{"location":"datagat/","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Data Gathering Getting Started Instructions to Create Model-Ready Dataset Create a Conda virtual environment with all the packages installed by following the directions below. A note that if you have a mac, some of the packages like GDAL might be harder to install. Make sure you have the miniconda shell or another type of terminal installed. Type conda info --envs to see the other environments that exist and to verify you are in the base environment. Then type conda env create tovaperlman/itu_test_02 . It should take a few moments to minutes for all the packages to download. Once this is done, you can check that it's an environment present in your computer by typing again conda info --envs Then to activate the environment you can type conda activate itu_test_02 Once you've activated the environment which has all the necessary packages and dependencies installed, navigate in the same terminal to wherever you've saved the repository. Within that navigate to src/scripts/map_offline/. Then open the repository within a code editor (Visual Studio Code, Atom, Pycharm, etc.) To generate a dataframe comprising any target variables and predictors that you wish to use, first set up the use case configurations in feature_engineering/configs.py. Details of the configurable variables and their expected assignments can be found in the 'Meta Data Information/ configurations' section of the documentation. For this, you must separately get access tokens for open cell ID, Google Earth Engine, and Facebook API. Once you have these, log all of them in the configs file. Otherwise, you will get errors when trying to run the scripts. For Open Cell ID , you just need to sign up for an account and it will give you an API Access Token. For Google Earth Engine, you must first sign up for a Google Earth Engine account . You cannot use Google Earth Engine unless your application has been approved. Once you receive the application approval email, you can log in to the Earth Engine Code Editor to get familiar with the JavaScript API. For Facebook API Access Token, follow the directions from the World Bank here Having correctly set the desired configurations, all you need to do is run 'main.py'. Following this, a dataset for model training and/or application will be saved within a training_sets folder, which is situated within the data directory. Internal Data Surveys from ITU for Brazil and Thailand The target variable for our modeling was the proportion of a population around a particular school that was connected to the internet. It therefore ranged from 0-1, with 0 being zero percent connected and 100 being 100% connected to the internet. We chose to measure this on a school level as one of our objectives, through working with UNICEF, was to detect schools that could be connected to the internet and further serve the community they are located. Within the Brazil survey data, we received information on household internet connectivity on an enumeration area level. This presented a slight challenge as the level of granularity of the school data was slightly different from the enumeration data or census tract. Thus, we matched the school points to the enumeration area data. We could not use all the school points as we only had enumeration areas for a specific amount of tracts in Brazil. Thus we had to subset our school points data to around 11,000 points. Once we connected the schools to the enumeration areas we were able to build our training data set. School Points from UNICEF for Brazil We got the school points in lat, long format from UNICEF for Brazil. Unfortunately, we were not able to obtain the school points for Thailand. We turned to OpenStreetMap to obtain school points for Thailand. We obtained many school points but filtered them to the schools that we were positive were schools as some were tagged as dance schools or even ATM's. Our school points script is specific for obtaining the OSM points. External/Open Source Data Brief note on licensing: Most of the data sources we used are open source and for research or educational purposes only and not for commercial use. OpenStreetMap\u00ae is open data, licensed under the Open Data Commons Open Database License (ODbL) by the OpenStreetMap Foundation (OSMF). Google Earth Engine Terms of Service Dataset Descriptions OpenCelliD Data OpenCelliD is a collaborative community project that collects GPS positions of cell towers and their corresponding location area identity. The dataset includes the locations and types of cell towers which is used to calculate proximity of a tower to a school location. Each cell tower location contains the following adjoining attributes: Parameter Description radio Network type. One of the strings GSM, UMTS, LTE or CDMA. mcc Mobile Country Code (UK: 234, 235) net Mobile Network Code (MNC) area Location Area Code (LAC) for GSM and UMTS networks. Tracking Area Code (TAC) for LTE networks. Network Idenfitication number (NID) for CDMA networks cell Cell ID unit Primary Scrambling Code (PSC) for UMTS networks. Physical Cell ID (PCI) for LTE networks. An empty value for GSM and CDMA networks lon Longitude in degrees between -180.0 and 180.0 If changeable=1: average of longitude values of all related measurements. If changeable=0: exact GPS position of the cell tower lat Latitude in degrees between -90.0 and 90.0 If changeable=1: average of latitude values of all related measurements. If changeable=0: exact GPS position of the tower range Estimate of cell range, in meters. samples Total number of measurements assigned to the cell tower changeable Defines if coordinates of the cell tower are exact or approximate. created The first time when the cell tower was seen and added to the OpenCellID database. updated The last time when the cell tower was seen and updated. averageSignal Average signal strength from all assigned measurements for the cell. Population Data Population data is high-resolution geospatial data on population distributions. There are several types of gridded population count datasets in the WorldPop Open Population Repository (WOPR). In our data gathering pipeline we used Population Counts / Unconstrained individual countries 2000-2020 UN adjusted (1km resolution) datasets from WOPR. The dataset for individual countries is available in Geotiff and ASCII XYZ format at a resolution of 30 arc (approximately 1km at the equator). We used the Geotiff image format as input and process the image to get population counts and locations for each pixel in the image. Each pixel contains the following adjoining attributes: Field Name Type Description population Float UN adjusted population count for the pixel location. geometry Geometry The geometry representing the pixel point location. Satellite Data To see the full code for gathering this data, click here. To gather the satellite data, we used Google Earth Engine API for Python. We gathered three different types of data: Global Human Modification Index, Nighttime Data, and Normalized Difference Vegetation Index. Our hope with gathering this data is that it would provide an accurate proxy for households and schools with internet connection. If we knew a school was located in a place with a high average radiance, it might also mean there was high internet connectivity. The beauty of satellite data is that its continous for the entire globe. We initially struggled with learning how to crop the data for all the school points we wanted. Eventually, we set a buffer, 5 kilometers in our case, zone around each school point in both Brazil and Thailand and obtained specific satellite information that was input as a number into the training dataset. Below please find more information on each of the datasets we used including descriptions taken from the Google Earth Engine Data Catalog. Global Human Modification Index (String for Image Collection ID is: 'CSP/HM/GlobalHumanModification'): The global Human Modification dataset (gHM) provides a cumulative measure of human modification of terrestrial lands globally at 1 square-kilometer resolution. The gHM values range from 0.0-1.0 and are calculated by estimating the proportion of a given location (pixel) that is modified, the estimated intensity of modification associated with a given type of human modification or \"stressor\". 5 major anthropogenic stressors circa 2016 were mapped using 13 individual datasets: human settlement (population density, built-up areas) agriculture (cropland, livestock) transportation (major, minor, and two-track roads; railroads) mining and energy production electrical infrastructure (power lines, nighttime lights) NOAA Monthly Nighttime images using the VIIRS Satellite (String for Image Collection ID is: \"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\") Monthly average radiance composite images using nighttime data from the Visible Infrared Imaging Radiometer Suite (VIIRS) Day/Night Band (DNB). As these data are composited monthly, there are many areas of the globe where it is impossible to get good quality data coverage for that month. This can be due to cloud cover, especially in the tropical regions, or due to solar illumination, as happens toward the poles in their respective summer months. Therefore it is recommended that users of these data utilize the 'cf_cvg' band and not assume a value of zero in the average radiance image means that no lights were observed. Normalized Difference Vegetation Index Band from the MODIS dataset (String for Image Collection ID is: 'MODIS/006/MOD13A2'): Normalized Difference Vegetation Index or NDVI measures the vegetation or greenness present on the Earth's surface The algorithm for this product chooses the best available pixel value from all the acquisitions from the 16-day period. The criteria used are low clouds, low view angle, and the highest NDVI/EVI value. Each school location contains the following adjoining attributes: Field Name Type Description mean_ghm Float The average global human modification index for the location and year 2016 mean_avg_rad Float The average radiance for the location and given start year and end year change_year_avg_rad Float The average yearly change of radiencebetween start year and end year for the location slope_year_avg_rad Float The average yearly slope of change of radiance between start year and end year for the location change_month_avg_rad Float The average monthly change of radiance between start year and end year for the location slope_month_avg_rad Float The average monthly slope of change of radiance between start year and end year for the location mean_cf_cvg Float The average cloud free coverage for the location and given start year and end year change_year_cf_cvg Float The average yearly change of cloud free coverage between start year and end year for the location slope_year_cf_cvg Float The average yearly slope of change of cloud free coverage between start year and end year for the location change_month_cf_cvg Float The average monthly change of cloud free coverage between start year and end year for the location slope_month_cf_cvg Float The average monthly slope of change of cloud free coverage between start year and end year for the location mean_NDVI Float The average NDVI for the location and given start year and end year change_year_NDVI Float The average yearly change of NDVI between start year and end year for the location slope_year_NDVI Float The average yearly slope of change of NDVI between start year and end year for the location change_month_NDVI Float The average monthly change of NDVI between start year and end year for the location slope_month_NDVI Float The average monthly slope of change of NDVI between start year and end year for the location Facebook Data Facebook data refers to the data that we get from the Facebook Marketing API . The Marketing API is an HTTP-based API that you can use to programmatically query data, create and manage ads, and perform a wide variety of other tasks. Furthermore, our Facebook data mainly uses the Ads Management API under the Marketing API which has the method that provides a delivery estimate for a given ad set configuration. The Ad set refers to the collection of advertisements. For each ad set, it is possible to define delivery estimate using the Ad Set Delivery Estimate method of the API. Two parameters are required for the delivery estimate method in order to get delivery estimate for a given location: optimization goal and dictionary that defines targeting specifications. Optimization goal can take several values such as 'clicks', 'impressions', 'replies', 'reach' etc. In our case, we used 'reach' as an optimization goal parameter since it carries out the 'reach' objective to show ads to the maximum number of people in the area. On the other hand, the targeting specification parameter is nicely customizable with fields such as 'geo-locations', 'interests', 'genders', 'age', 'relationship_status' and so on. In our case, we use custom locations (schools) with radius (5 kilometers) as our targeting specification and collect reach estimates for those locations. Each custom location contains the following adjoining attributes: Field Name Type Description estimate_dau Integer The estimated number of people that have been active on your selected platforms and satisfy your targeting spec in the past day. estimate_mau Integer The estimated number of people that have been active on your selected platforms and satisfy your targeting spec in the past month. estimate_ready Boolean Whether or not an estimate is ready for the audience. Some audiences require time to populate before we can provide a delivery estimate. Speedtest Data Speedtest data provides global fixed broadband and mobile (cellular) network performance metrics. The dataset provided by Ookla Open Data Projects is in zoom level 16 web mercator tiles (approximately 610.8 meters by 610.8 meters at the equator). Data is provided in both Shapefile format as well as Apache Parquet with geometries represented in Well Known Text (WKT) projected in EPSG:4326. Download speed, upload speed, and latency are collected via the Speedtest by Ookla applications and averaged for each tile. Each tile contains the following adjoining attributes: Field Name Type Description avg_d_kbps Integer The average download speed of all tests performed in the tile, represented in kilobits per second. avg_u_kbps Integer The average upload speed of all tests performed in the tile, represented in kilobits per second. avg_lat_ms Integer The average latency of all tests performed in the tile, represented in milliseconds tests Integer The number of tests taken in the tile. devices Integer The number of unique devices contributing tests in the tile. quadkey Text The quadkey representing the tile. geometry Geometry The geometry representing the tile location and shape. Data Gathering and Feature Engineering Class Diagram Our data pipeline includes three superclasses: Country, Opendata and Feature Engineering. Country is a parent class to our school and survey classes. Opendata class is a parent class to speedtest, opencell, facebook, population and satellite open-source data classes. Finally, we coded feature engineering class in which school, survey and opendata classes are processed and merged. Map Offline Package Hierarchy: classDiagram map_offline < |-- OpenData map_offline < |-- FeatureEngineering map_offline < |-- Country Country < |-- School Country < |-- Survey Survey < |-- BRA_Survey Survey < |-- THA_Survey Survey < |-- PHL_Survey OpenData < |-- PopulationData OpenData < |-- SpeedtestData OpenData < |-- FacebookData OpenData < |-- OpencellData OpenData < |-- SatelliteData class map_offline{ +training_set_vxxx } class FeatureEngineering{ + configs + school_data + set_training_data() + save_training_set() + get_opendata() } class Country{ + country_code + country_name + geodata + set_country_geometry() } class School{ + buffer + data + set_school_data() } class Survey{ + available_countries + data } class BRA_Survey{ + set_survey_data() } class THA_Survey{ + set_survey_data() } class PHL_Survey{ + set_survey_data() } class OpenData{ + country_code + data } class PopulationData{ + year + set_pop_data() } class SpeedtestData{ + type + year + quarter + set_speedtest_data() } class FacebookData{ + locations + access_token + ad_account_id + call_limit + radius + set_fb_data() } class OpencellData{ + access_token + set_cell_data() } class SatelliteData{ + locations + start_year + end_year + buffer + max_call_size + json_key_path + ee_service_account + set_satellite_data() } Data Gathering Pipeline We are still working on the data gathering pipeline workflow... OpencellData PopulationData SatelliteData FacebookData SpeedtestData Training Data Dictionary Show a table of each of the predictor in the training set and what their definitions are: Variable Name Description Data Source avg_d_kbps Average Download Speed Speedtest Data avg_u_kbps Average Upload Speed Speedtest Data estimate_dau Facebook Daily Active Users estimate Facebook Data estimate_mau Facebook Monthly Active Users estimate Facebook Data population Population around school buffer zone Population Data mean_ghm Mean Global Human Modification value Satellite Data - Global Human Modification Index mean_avg_rad Mean value from the Average Radiance band Satellite Data - VIIRS Nighttime DNB mean_cf_cvg Mean value from the cloud free coverage band Satellite Data - VIIRS Nighttime DNB slope_year_avg_rad The yearly rate of change between 2019 and 2014 of Average Radiance Satellite Data - VIIRS Nighttime DNB change_year_avg_rad The change between the average values of 2019 and 2014 of Average Radiance Satellite Data - VIIRS Nighttime DNB slope_year_cf_cvg The yearly rate of change between 2019 and 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB change_year_cf_cvg The change between the average values of 2019 and 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB slope_month_avg_rad The monthly rate of change between 2019 and 2014 of the Average Radiance Band Satellite Data - VIIRS Nighttime DNB change_month_avg_rad The change between the average of Dec 2019 and Jan 2014 of the Average Radiance Band Satellite Data - VIIRS Nighttime DNB slope_month_cf_cvg The monthly rate of change between 2019 and 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB change_month_cf_cvg The rate of change between the average of Dec 2019 and Jan 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB mean_NDVI The average value of the Vegetation Index Satellite Data - MODIS Dataset slope_year_NDVI The yearly rate of change between 2019 and 2014 of the Vegetation Index Satellite Data - MODIS Dataset change_year_NDVI The change between 2019 and 2014 of the Vegetation Index Satellite Data - MODIS Dataset slope_month_NDVI The monthly rate of change between 2019 and 2014 of the Vegetation Index Satellite Data - MODIS Dataset change_month_NDVI The change between the average of May 2019 and May 2014 of the Vegetation Index Satellite Data - MODIS Dataset range The binary variable that checks whether there is opencell tower in the to the school area OpenCelliD Data","title":"Data Gathering"},{"location":"datagat/#data-gathering","text":"","title":"Data Gathering"},{"location":"datagat/#getting-started","text":"","title":"Getting Started"},{"location":"datagat/#instructions-to-create-model-ready-dataset","text":"Create a Conda virtual environment with all the packages installed by following the directions below. A note that if you have a mac, some of the packages like GDAL might be harder to install. Make sure you have the miniconda shell or another type of terminal installed. Type conda info --envs to see the other environments that exist and to verify you are in the base environment. Then type conda env create tovaperlman/itu_test_02 . It should take a few moments to minutes for all the packages to download. Once this is done, you can check that it's an environment present in your computer by typing again conda info --envs Then to activate the environment you can type conda activate itu_test_02 Once you've activated the environment which has all the necessary packages and dependencies installed, navigate in the same terminal to wherever you've saved the repository. Within that navigate to src/scripts/map_offline/. Then open the repository within a code editor (Visual Studio Code, Atom, Pycharm, etc.) To generate a dataframe comprising any target variables and predictors that you wish to use, first set up the use case configurations in feature_engineering/configs.py. Details of the configurable variables and their expected assignments can be found in the 'Meta Data Information/ configurations' section of the documentation. For this, you must separately get access tokens for open cell ID, Google Earth Engine, and Facebook API. Once you have these, log all of them in the configs file. Otherwise, you will get errors when trying to run the scripts. For Open Cell ID , you just need to sign up for an account and it will give you an API Access Token. For Google Earth Engine, you must first sign up for a Google Earth Engine account . You cannot use Google Earth Engine unless your application has been approved. Once you receive the application approval email, you can log in to the Earth Engine Code Editor to get familiar with the JavaScript API. For Facebook API Access Token, follow the directions from the World Bank here Having correctly set the desired configurations, all you need to do is run 'main.py'. Following this, a dataset for model training and/or application will be saved within a training_sets folder, which is situated within the data directory.","title":"Instructions to Create Model-Ready Dataset"},{"location":"datagat/#internal-data","text":"Surveys from ITU for Brazil and Thailand The target variable for our modeling was the proportion of a population around a particular school that was connected to the internet. It therefore ranged from 0-1, with 0 being zero percent connected and 100 being 100% connected to the internet. We chose to measure this on a school level as one of our objectives, through working with UNICEF, was to detect schools that could be connected to the internet and further serve the community they are located. Within the Brazil survey data, we received information on household internet connectivity on an enumeration area level. This presented a slight challenge as the level of granularity of the school data was slightly different from the enumeration data or census tract. Thus, we matched the school points to the enumeration area data. We could not use all the school points as we only had enumeration areas for a specific amount of tracts in Brazil. Thus we had to subset our school points data to around 11,000 points. Once we connected the schools to the enumeration areas we were able to build our training data set. School Points from UNICEF for Brazil We got the school points in lat, long format from UNICEF for Brazil. Unfortunately, we were not able to obtain the school points for Thailand. We turned to OpenStreetMap to obtain school points for Thailand. We obtained many school points but filtered them to the schools that we were positive were schools as some were tagged as dance schools or even ATM's. Our school points script is specific for obtaining the OSM points.","title":"Internal Data"},{"location":"datagat/#externalopen-source-data","text":"Brief note on licensing: Most of the data sources we used are open source and for research or educational purposes only and not for commercial use. OpenStreetMap\u00ae is open data, licensed under the Open Data Commons Open Database License (ODbL) by the OpenStreetMap Foundation (OSMF). Google Earth Engine Terms of Service","title":"External/Open Source Data"},{"location":"datagat/#dataset-descriptions","text":"OpenCelliD Data OpenCelliD is a collaborative community project that collects GPS positions of cell towers and their corresponding location area identity. The dataset includes the locations and types of cell towers which is used to calculate proximity of a tower to a school location. Each cell tower location contains the following adjoining attributes: Parameter Description radio Network type. One of the strings GSM, UMTS, LTE or CDMA. mcc Mobile Country Code (UK: 234, 235) net Mobile Network Code (MNC) area Location Area Code (LAC) for GSM and UMTS networks. Tracking Area Code (TAC) for LTE networks. Network Idenfitication number (NID) for CDMA networks cell Cell ID unit Primary Scrambling Code (PSC) for UMTS networks. Physical Cell ID (PCI) for LTE networks. An empty value for GSM and CDMA networks lon Longitude in degrees between -180.0 and 180.0 If changeable=1: average of longitude values of all related measurements. If changeable=0: exact GPS position of the cell tower lat Latitude in degrees between -90.0 and 90.0 If changeable=1: average of latitude values of all related measurements. If changeable=0: exact GPS position of the tower range Estimate of cell range, in meters. samples Total number of measurements assigned to the cell tower changeable Defines if coordinates of the cell tower are exact or approximate. created The first time when the cell tower was seen and added to the OpenCellID database. updated The last time when the cell tower was seen and updated. averageSignal Average signal strength from all assigned measurements for the cell. Population Data Population data is high-resolution geospatial data on population distributions. There are several types of gridded population count datasets in the WorldPop Open Population Repository (WOPR). In our data gathering pipeline we used Population Counts / Unconstrained individual countries 2000-2020 UN adjusted (1km resolution) datasets from WOPR. The dataset for individual countries is available in Geotiff and ASCII XYZ format at a resolution of 30 arc (approximately 1km at the equator). We used the Geotiff image format as input and process the image to get population counts and locations for each pixel in the image. Each pixel contains the following adjoining attributes: Field Name Type Description population Float UN adjusted population count for the pixel location. geometry Geometry The geometry representing the pixel point location. Satellite Data To see the full code for gathering this data, click here. To gather the satellite data, we used Google Earth Engine API for Python. We gathered three different types of data: Global Human Modification Index, Nighttime Data, and Normalized Difference Vegetation Index. Our hope with gathering this data is that it would provide an accurate proxy for households and schools with internet connection. If we knew a school was located in a place with a high average radiance, it might also mean there was high internet connectivity. The beauty of satellite data is that its continous for the entire globe. We initially struggled with learning how to crop the data for all the school points we wanted. Eventually, we set a buffer, 5 kilometers in our case, zone around each school point in both Brazil and Thailand and obtained specific satellite information that was input as a number into the training dataset. Below please find more information on each of the datasets we used including descriptions taken from the Google Earth Engine Data Catalog. Global Human Modification Index (String for Image Collection ID is: 'CSP/HM/GlobalHumanModification'): The global Human Modification dataset (gHM) provides a cumulative measure of human modification of terrestrial lands globally at 1 square-kilometer resolution. The gHM values range from 0.0-1.0 and are calculated by estimating the proportion of a given location (pixel) that is modified, the estimated intensity of modification associated with a given type of human modification or \"stressor\". 5 major anthropogenic stressors circa 2016 were mapped using 13 individual datasets: human settlement (population density, built-up areas) agriculture (cropland, livestock) transportation (major, minor, and two-track roads; railroads) mining and energy production electrical infrastructure (power lines, nighttime lights) NOAA Monthly Nighttime images using the VIIRS Satellite (String for Image Collection ID is: \"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\") Monthly average radiance composite images using nighttime data from the Visible Infrared Imaging Radiometer Suite (VIIRS) Day/Night Band (DNB). As these data are composited monthly, there are many areas of the globe where it is impossible to get good quality data coverage for that month. This can be due to cloud cover, especially in the tropical regions, or due to solar illumination, as happens toward the poles in their respective summer months. Therefore it is recommended that users of these data utilize the 'cf_cvg' band and not assume a value of zero in the average radiance image means that no lights were observed. Normalized Difference Vegetation Index Band from the MODIS dataset (String for Image Collection ID is: 'MODIS/006/MOD13A2'): Normalized Difference Vegetation Index or NDVI measures the vegetation or greenness present on the Earth's surface The algorithm for this product chooses the best available pixel value from all the acquisitions from the 16-day period. The criteria used are low clouds, low view angle, and the highest NDVI/EVI value. Each school location contains the following adjoining attributes: Field Name Type Description mean_ghm Float The average global human modification index for the location and year 2016 mean_avg_rad Float The average radiance for the location and given start year and end year change_year_avg_rad Float The average yearly change of radiencebetween start year and end year for the location slope_year_avg_rad Float The average yearly slope of change of radiance between start year and end year for the location change_month_avg_rad Float The average monthly change of radiance between start year and end year for the location slope_month_avg_rad Float The average monthly slope of change of radiance between start year and end year for the location mean_cf_cvg Float The average cloud free coverage for the location and given start year and end year change_year_cf_cvg Float The average yearly change of cloud free coverage between start year and end year for the location slope_year_cf_cvg Float The average yearly slope of change of cloud free coverage between start year and end year for the location change_month_cf_cvg Float The average monthly change of cloud free coverage between start year and end year for the location slope_month_cf_cvg Float The average monthly slope of change of cloud free coverage between start year and end year for the location mean_NDVI Float The average NDVI for the location and given start year and end year change_year_NDVI Float The average yearly change of NDVI between start year and end year for the location slope_year_NDVI Float The average yearly slope of change of NDVI between start year and end year for the location change_month_NDVI Float The average monthly change of NDVI between start year and end year for the location slope_month_NDVI Float The average monthly slope of change of NDVI between start year and end year for the location Facebook Data Facebook data refers to the data that we get from the Facebook Marketing API . The Marketing API is an HTTP-based API that you can use to programmatically query data, create and manage ads, and perform a wide variety of other tasks. Furthermore, our Facebook data mainly uses the Ads Management API under the Marketing API which has the method that provides a delivery estimate for a given ad set configuration. The Ad set refers to the collection of advertisements. For each ad set, it is possible to define delivery estimate using the Ad Set Delivery Estimate method of the API. Two parameters are required for the delivery estimate method in order to get delivery estimate for a given location: optimization goal and dictionary that defines targeting specifications. Optimization goal can take several values such as 'clicks', 'impressions', 'replies', 'reach' etc. In our case, we used 'reach' as an optimization goal parameter since it carries out the 'reach' objective to show ads to the maximum number of people in the area. On the other hand, the targeting specification parameter is nicely customizable with fields such as 'geo-locations', 'interests', 'genders', 'age', 'relationship_status' and so on. In our case, we use custom locations (schools) with radius (5 kilometers) as our targeting specification and collect reach estimates for those locations. Each custom location contains the following adjoining attributes: Field Name Type Description estimate_dau Integer The estimated number of people that have been active on your selected platforms and satisfy your targeting spec in the past day. estimate_mau Integer The estimated number of people that have been active on your selected platforms and satisfy your targeting spec in the past month. estimate_ready Boolean Whether or not an estimate is ready for the audience. Some audiences require time to populate before we can provide a delivery estimate. Speedtest Data Speedtest data provides global fixed broadband and mobile (cellular) network performance metrics. The dataset provided by Ookla Open Data Projects is in zoom level 16 web mercator tiles (approximately 610.8 meters by 610.8 meters at the equator). Data is provided in both Shapefile format as well as Apache Parquet with geometries represented in Well Known Text (WKT) projected in EPSG:4326. Download speed, upload speed, and latency are collected via the Speedtest by Ookla applications and averaged for each tile. Each tile contains the following adjoining attributes: Field Name Type Description avg_d_kbps Integer The average download speed of all tests performed in the tile, represented in kilobits per second. avg_u_kbps Integer The average upload speed of all tests performed in the tile, represented in kilobits per second. avg_lat_ms Integer The average latency of all tests performed in the tile, represented in milliseconds tests Integer The number of tests taken in the tile. devices Integer The number of unique devices contributing tests in the tile. quadkey Text The quadkey representing the tile. geometry Geometry The geometry representing the tile location and shape.","title":"Dataset Descriptions"},{"location":"datagat/#data-gathering-and-feature-engineering-class-diagram","text":"Our data pipeline includes three superclasses: Country, Opendata and Feature Engineering. Country is a parent class to our school and survey classes. Opendata class is a parent class to speedtest, opencell, facebook, population and satellite open-source data classes. Finally, we coded feature engineering class in which school, survey and opendata classes are processed and merged.","title":"Data Gathering and Feature Engineering Class Diagram"},{"location":"datagat/#map-offline-package-hierarchy","text":"classDiagram map_offline < |-- OpenData map_offline < |-- FeatureEngineering map_offline < |-- Country Country < |-- School Country < |-- Survey Survey < |-- BRA_Survey Survey < |-- THA_Survey Survey < |-- PHL_Survey OpenData < |-- PopulationData OpenData < |-- SpeedtestData OpenData < |-- FacebookData OpenData < |-- OpencellData OpenData < |-- SatelliteData class map_offline{ +training_set_vxxx } class FeatureEngineering{ + configs + school_data + set_training_data() + save_training_set() + get_opendata() } class Country{ + country_code + country_name + geodata + set_country_geometry() } class School{ + buffer + data + set_school_data() } class Survey{ + available_countries + data } class BRA_Survey{ + set_survey_data() } class THA_Survey{ + set_survey_data() } class PHL_Survey{ + set_survey_data() } class OpenData{ + country_code + data } class PopulationData{ + year + set_pop_data() } class SpeedtestData{ + type + year + quarter + set_speedtest_data() } class FacebookData{ + locations + access_token + ad_account_id + call_limit + radius + set_fb_data() } class OpencellData{ + access_token + set_cell_data() } class SatelliteData{ + locations + start_year + end_year + buffer + max_call_size + json_key_path + ee_service_account + set_satellite_data() }","title":"Map Offline Package Hierarchy:"},{"location":"datagat/#data-gathering-pipeline","text":"We are still working on the data gathering pipeline workflow... OpencellData PopulationData SatelliteData FacebookData SpeedtestData","title":"Data Gathering Pipeline"},{"location":"datagat/#training-data-dictionary","text":"Show a table of each of the predictor in the training set and what their definitions are: Variable Name Description Data Source avg_d_kbps Average Download Speed Speedtest Data avg_u_kbps Average Upload Speed Speedtest Data estimate_dau Facebook Daily Active Users estimate Facebook Data estimate_mau Facebook Monthly Active Users estimate Facebook Data population Population around school buffer zone Population Data mean_ghm Mean Global Human Modification value Satellite Data - Global Human Modification Index mean_avg_rad Mean value from the Average Radiance band Satellite Data - VIIRS Nighttime DNB mean_cf_cvg Mean value from the cloud free coverage band Satellite Data - VIIRS Nighttime DNB slope_year_avg_rad The yearly rate of change between 2019 and 2014 of Average Radiance Satellite Data - VIIRS Nighttime DNB change_year_avg_rad The change between the average values of 2019 and 2014 of Average Radiance Satellite Data - VIIRS Nighttime DNB slope_year_cf_cvg The yearly rate of change between 2019 and 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB change_year_cf_cvg The change between the average values of 2019 and 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB slope_month_avg_rad The monthly rate of change between 2019 and 2014 of the Average Radiance Band Satellite Data - VIIRS Nighttime DNB change_month_avg_rad The change between the average of Dec 2019 and Jan 2014 of the Average Radiance Band Satellite Data - VIIRS Nighttime DNB slope_month_cf_cvg The monthly rate of change between 2019 and 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB change_month_cf_cvg The rate of change between the average of Dec 2019 and Jan 2014 from the Cloud Free Coverage Band Satellite Data - VIIRS Nighttime DNB mean_NDVI The average value of the Vegetation Index Satellite Data - MODIS Dataset slope_year_NDVI The yearly rate of change between 2019 and 2014 of the Vegetation Index Satellite Data - MODIS Dataset change_year_NDVI The change between 2019 and 2014 of the Vegetation Index Satellite Data - MODIS Dataset slope_month_NDVI The monthly rate of change between 2019 and 2014 of the Vegetation Index Satellite Data - MODIS Dataset change_month_NDVI The change between the average of May 2019 and May 2014 of the Vegetation Index Satellite Data - MODIS Dataset range The binary variable that checks whether there is opencell tower in the to the school area OpenCelliD Data","title":"Training Data Dictionary"},{"location":"eda/","text":"Exploratory Data Analysis As a disclaimer, some of this EDA is to explore our raw data and what it looks like. However, to run these notebooks, especially with using the satellite data and school points you will already need to run the Data Gathering and Feature Engineering Scripts first. Satellite Data The first thing we wanted to explore in our Exploratory Data Analysis was some maps of what our countries looked like and how our predictors might map onto our countries. We used Google Earth Engine to create some maps of nighttime imagery, the global human modification index and the vegetation index. For nighttime and vegetation index, we also wanted to show the change in time as we were using the rate of change as a predictor as well. Below you will find some static images of the maps we created. If you click on them, you can also find an interactive version. Click here to see the Jupyter notebook with code included for replicating the maps below. Satellite Images on a National Level for both Brazil and Thailand: Average Radiance Band Here we see that the Average light comes from the big cities in the south for both countries. This predictor later plays a big role in determining internet connectivity. Click on this map to see a comparison between school points and the entire country average radiance in 2014 and in 2019. Cloud Free Band This is a second band within the VIIRS Satellite nighttime images. It measures light without clouds or solar illumination. In some ways, specifically in tropical rainforests which both Brazil and Thailand have, it is a better measure of light emittance than the average radiance band. We use both as predictors in our model. Additionally, you see in the maps that the light emittance looks vastly different. Click on this map to see a comparison between school points and the entire country cloud free coverage in 2014 and in 2019. Global Human Modification Map In this map, we see the level of Global Human Modification in the last few years within both Brazil and Thailand. For more information on how this dataset was compiled, please see the Data Gathering page. Click on this Brazil Map to see the country level data. Normalized Difference Vegetation Index Here we see the difference in vegetation between Brazil and Thailand. Click here to see the map for Brazil, toggle between the layers to see the entire country and just the school point areas. Here we also see GIFs that show the time series change of vegetation from 2000 to 2021. Speedtest data Open Cell ID data Facebook Data Training Set EDA We also did some Exploratory Data Analysis once our training dataset was created. You will not be able to run this on your own until you have run the Data Gathering and Feature Engineering scripts. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file.","title":"Exploratory Data Analysis"},{"location":"eda/#exploratory-data-analysis","text":"As a disclaimer, some of this EDA is to explore our raw data and what it looks like. However, to run these notebooks, especially with using the satellite data and school points you will already need to run the Data Gathering and Feature Engineering Scripts first.","title":"Exploratory Data Analysis"},{"location":"eda/#satellite-data","text":"The first thing we wanted to explore in our Exploratory Data Analysis was some maps of what our countries looked like and how our predictors might map onto our countries. We used Google Earth Engine to create some maps of nighttime imagery, the global human modification index and the vegetation index. For nighttime and vegetation index, we also wanted to show the change in time as we were using the rate of change as a predictor as well. Below you will find some static images of the maps we created. If you click on them, you can also find an interactive version. Click here to see the Jupyter notebook with code included for replicating the maps below. Satellite Images on a National Level for both Brazil and Thailand: Average Radiance Band Here we see that the Average light comes from the big cities in the south for both countries. This predictor later plays a big role in determining internet connectivity. Click on this map to see a comparison between school points and the entire country average radiance in 2014 and in 2019. Cloud Free Band This is a second band within the VIIRS Satellite nighttime images. It measures light without clouds or solar illumination. In some ways, specifically in tropical rainforests which both Brazil and Thailand have, it is a better measure of light emittance than the average radiance band. We use both as predictors in our model. Additionally, you see in the maps that the light emittance looks vastly different. Click on this map to see a comparison between school points and the entire country cloud free coverage in 2014 and in 2019. Global Human Modification Map In this map, we see the level of Global Human Modification in the last few years within both Brazil and Thailand. For more information on how this dataset was compiled, please see the Data Gathering page. Click on this Brazil Map to see the country level data. Normalized Difference Vegetation Index Here we see the difference in vegetation between Brazil and Thailand. Click here to see the map for Brazil, toggle between the layers to see the entire country and just the school point areas. Here we also see GIFs that show the time series change of vegetation from 2000 to 2021.","title":"Satellite Data"},{"location":"eda/#speedtest-data","text":"","title":"Speedtest data"},{"location":"eda/#open-cell-id-data","text":"","title":"Open Cell ID data"},{"location":"eda/#facebook-data","text":"","title":"Facebook Data"},{"location":"eda/#training-set-eda","text":"We also did some Exploratory Data Analysis once our training dataset was created. You will not be able to run this on your own until you have run the Data Gathering and Feature Engineering scripts. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file.","title":"Training Set EDA"},{"location":"fe/","text":"mermaid.initialize({startOnLoad:true}); mermaidAPI.initialize({ securityLevel: 'loose' }); Feature Engineering Having retrieved data of many different types, at different geospatial resolutions, from an array of different sources, it was necessary to develop a robust and comprehensive feature engineering pipeline, which produces clean datasets, ready for model training or application. Data Cleaning Numerical Variables - Impute missing values as variable median. Categorical Variables - Fill missing values with 'missing' label; perform one-hot encoding. Target Variable Our target variable is ground-truth survey data on local internet connectivity. For the Brazilian and Thai surveys, forwarded to us by ITU, these target variables correspond to the following labels: Brazil - A4A Thailand - H107 In each case, we rename our target variable to simply 'target'. Joining Locations In the following description, we use the word 'feature' to mean predictor and, in some cases, ground truth survey variable. The map_feature method within the FeatureEngineering class is used to perform a spatial join between school locations and feature values. In essence, we are finding the correct value of each feature at each given school location. To do so, we first ensure that the dataframe corresponding to each predictor or survey dataset is loaded as a geodataframe, with an appropriately defined 'geometry' column. If the feature's geometry is a polygon, or multipolygon, rather than a point, we take the centroid as the location with which to match. If, instead, the feature's geometry is defined by latitude and longitude columns, rather than a single geometry column, this will be handled appropriately, so long as these column names are exactly 'latitude' and 'longitude', or 'lat' and 'lon'. For the unexpected instance in which schools have already been matched to feature values, we look for a 'source_school_id' column and merge the school data to the feature data according to these school ids. This is for the sake of code robustness, in case features that are already mapped to schools are mistakenly passed to the map_feature method. The build_tree method is then called to implement Scikit-learn's KDTree package and thus build a kd tree of the previously defined centroids. The kd tree is a tree in k-dimensional space; for us, this defines the spatial relationships between locations. This tree is then queried with the school locations to get the nearest neighbours, with the query returning dist (geographic distance) and ind (index associated with this location). The rows in the school_data dataframe are then assigned the correct values for each new column. For any variable named 'range', such as the mobile cell tower range variable from the OpenCellID dataset, values are converted to a 0 or 1, corresponding to range < dist and range >= dist respectively. This last part converts any 'range' to a binary variable, representing out-of-range, or in-range. If Dataset Is Sparsely Scattered across Country: It should be noted that the map_feature method should only be used for features that are not sparsely distributed. For example, this method is not used for the Brazil survey data, which is only available for a selection of enumeration areas, which often have significant geographic regions between them. For other features, such as the Brazil survey data, which have values only in sparsely distributed locations, we employ the map_enumeration method, which joins school locations to areas via intersections between the enumeration area polygons and 1km radius school buffer zones. We then check for instances in which a school might have been joined to multiple enumeration areas and select only the nearest enumeration area for such cases. High Level Feature Engineering Pipeline graph TD A[Get School Data] --> B{Is Survey Available?}; B --> |Yes| C[Get Survey Data]; B --> |No| D[Load Predictor Dataset]; C --> E D --> E[Initialise New Columns]; E --> F[Clean New Data]; F --> G[Match New Data to Schools]; G --> H{All Features Added?}; H --> |Yes| I[Save Dataset] H --> |No| D; Classification Use Case Our problem is a regression problem, with an outcome that needs to be predictions from 0 -1. However, should you wish to train or apply a classifier, instead of a regression model, you can create a binary classification dataset by running the 'binning.py' script, found in the src/scripts/ folder. Within this script, it is straightforward to edit the name of a standard regression training dataset that you wish to convert into a classification-ready dataset. To train/apply a classification model, one would then simply need to change the dataset file name within the 'configs.yml' file.","title":"Feature Engineering"},{"location":"fe/#feature-engineering","text":"Having retrieved data of many different types, at different geospatial resolutions, from an array of different sources, it was necessary to develop a robust and comprehensive feature engineering pipeline, which produces clean datasets, ready for model training or application.","title":"Feature Engineering"},{"location":"fe/#data-cleaning","text":"Numerical Variables - Impute missing values as variable median. Categorical Variables - Fill missing values with 'missing' label; perform one-hot encoding.","title":"Data Cleaning"},{"location":"fe/#target-variable","text":"Our target variable is ground-truth survey data on local internet connectivity. For the Brazilian and Thai surveys, forwarded to us by ITU, these target variables correspond to the following labels: Brazil - A4A Thailand - H107 In each case, we rename our target variable to simply 'target'.","title":"Target Variable"},{"location":"fe/#joining-locations","text":"In the following description, we use the word 'feature' to mean predictor and, in some cases, ground truth survey variable. The map_feature method within the FeatureEngineering class is used to perform a spatial join between school locations and feature values. In essence, we are finding the correct value of each feature at each given school location. To do so, we first ensure that the dataframe corresponding to each predictor or survey dataset is loaded as a geodataframe, with an appropriately defined 'geometry' column. If the feature's geometry is a polygon, or multipolygon, rather than a point, we take the centroid as the location with which to match. If, instead, the feature's geometry is defined by latitude and longitude columns, rather than a single geometry column, this will be handled appropriately, so long as these column names are exactly 'latitude' and 'longitude', or 'lat' and 'lon'. For the unexpected instance in which schools have already been matched to feature values, we look for a 'source_school_id' column and merge the school data to the feature data according to these school ids. This is for the sake of code robustness, in case features that are already mapped to schools are mistakenly passed to the map_feature method. The build_tree method is then called to implement Scikit-learn's KDTree package and thus build a kd tree of the previously defined centroids. The kd tree is a tree in k-dimensional space; for us, this defines the spatial relationships between locations. This tree is then queried with the school locations to get the nearest neighbours, with the query returning dist (geographic distance) and ind (index associated with this location). The rows in the school_data dataframe are then assigned the correct values for each new column. For any variable named 'range', such as the mobile cell tower range variable from the OpenCellID dataset, values are converted to a 0 or 1, corresponding to range < dist and range >= dist respectively. This last part converts any 'range' to a binary variable, representing out-of-range, or in-range.","title":"Joining Locations"},{"location":"fe/#if-dataset-is-sparsely-scattered-across-country","text":"It should be noted that the map_feature method should only be used for features that are not sparsely distributed. For example, this method is not used for the Brazil survey data, which is only available for a selection of enumeration areas, which often have significant geographic regions between them. For other features, such as the Brazil survey data, which have values only in sparsely distributed locations, we employ the map_enumeration method, which joins school locations to areas via intersections between the enumeration area polygons and 1km radius school buffer zones. We then check for instances in which a school might have been joined to multiple enumeration areas and select only the nearest enumeration area for such cases.","title":"If Dataset Is Sparsely Scattered across Country:"},{"location":"fe/#high-level-feature-engineering-pipeline","text":"graph TD A[Get School Data] --> B{Is Survey Available?}; B --> |Yes| C[Get Survey Data]; B --> |No| D[Load Predictor Dataset]; C --> E D --> E[Initialise New Columns]; E --> F[Clean New Data]; F --> G[Match New Data to Schools]; G --> H{All Features Added?}; H --> |Yes| I[Save Dataset] H --> |No| D;","title":"High Level Feature Engineering Pipeline"},{"location":"fe/#classification-use-case","text":"Our problem is a regression problem, with an outcome that needs to be predictions from 0 -1. However, should you wish to train or apply a classifier, instead of a regression model, you can create a binary classification dataset by running the 'binning.py' script, found in the src/scripts/ folder. Within this script, it is straightforward to edit the name of a standard regression training dataset that you wish to convert into a classification-ready dataset. To train/apply a classification model, one would then simply need to change the dataset file name within the 'configs.yml' file.","title":"Classification Use Case"},{"location":"folder_structure/","text":"Folder Structure conf/ data/ geodata/ meta/ school_loc/ fb/ opencellid/ satellite/ speedtest/ survey/ Brazil/ Thailand/ Philippines/ training_sets/ Brazil/ Thailand/ Philippines/ worldpop/ model/ notebooks/ src/ scripts/ map_offline data_gathering init .py opendata.py opendata_utils.py opendata_scrap.py opendata_facebook.py opendata_satellite.py feature_engineering init .py data_pipeline.py configs.py country.py survey.py school.py main.py requirements.txt","title":"Folder Structure"},{"location":"folder_structure/#folder-structure","text":"conf/ data/ geodata/ meta/ school_loc/ fb/ opencellid/ satellite/ speedtest/ survey/ Brazil/ Thailand/ Philippines/ training_sets/ Brazil/ Thailand/ Philippines/ worldpop/ model/ notebooks/ src/ scripts/ map_offline data_gathering init .py opendata.py opendata_utils.py opendata_scrap.py opendata_facebook.py opendata_satellite.py feature_engineering init .py data_pipeline.py configs.py country.py survey.py school.py main.py requirements.txt","title":"Folder Structure"},{"location":"intro/","text":"Introduction Motivation Today, just over half of all people worldwide use the Internet. Yet in our fast-paced digital world, being offline excludes people from a host of opportunities. The Internet has grown beyond the remit of mere communication service and now grants access to global knowledge, social empowerment and new economic activities. In a word, the Internet fuels social and economic development and thus has been moving higher on national policy agendas around the world, all the more in the aftermath of the COVID-19 pandemic. Moreover, many countries lack data on Internet use in the society and can therefore not tackle the challenges most efficiently. Giga, the joint initiative by the International Telecommunication Union (ITU) and UNICEF, has set the goal to connect every school in the world to the Internet by 2030. Giga aims at giving children and youth the opportunity to get online and tap the potential of the Internet to their benefit. Connected, digitally savvy and enlightened youth can lead economies and societies towards a more productive, fulfilling, equal and sustainable future. What\u2019s more, as schools often serve as community hubs, the worth of connectivity can be extended to larger groups of the population. Arguably, connecting schools in no specific order does not allow for allocating resources where they are most needed and connecting communities that are most in need in a timely fashion. Giving priority to connecting schools that are closer to nodes or pro-actively apply for a connection may not serve national policy priorities best and could potentially even deepen gaps between urban and rural areas or across demographic groups, since the offline population is notoriously hard to target. This project therefore aims to provide national decision makers, multi-lateral agencies and development partners with a sound rationale for prioritization of schools to be connected, building on a large range of complementary datasets, advanced modelling and machine learning to identify the largest offline communities and craft connectivity roadmaps based on the most recent, relevant and accurate evidence available. Project objectives This pioneering project aims at developing mechanisms for predicting the world's offline population, using school locations as the starting point. A multi-disciplinary team of fellows hosted by the University of Warwick jointly with experts from ITU and UNICEF have examined three case studies, namely Brazil, Thailand and the Philippines. Based on Big Data and machine learning techniques, the team has created a toolbox that can be used by researchers and institutions to investigate the state of connectivity within a country and enable near real-time national and regional benchmarking. The project outcome can be readily reproduced for other countries and the individual models can be enhanced for specific purposes. The evidence generated by the models can support digital policy planning and implementation at the national level and inform the work of all national stakeholders and development partners involved in connecting the unconnected at the sub-national, national and regional levels. Concretely, the ambition of this project has been three-fold. First, to estimate the share of households/individuals with Internet connectivity around a school. Internet connectivity is defined here as any ability to get online, either through a fixed or mobile broadband connection. This also means that individuals must own or have access to broadband-enabled devices. Based on population data for the area around each school, the offline population across communities is estimated. A rank of priority can then be given to communities and a recommended order in which schools can be connected in view of optimal resource allocation and fast-tracking the achievement of social and economic policy objectives. In addition, quick wins can be identified through further modelling and analysis, also taking into account data on computer availability and electrification in schools. Secondly, based on the best estimates of community level connectivity, to aggregate Internet connectivity data up to the country level. While such aggregate figures might already exist for some countries, this bottom-up approach will be helpful in countries where there are currently no census or surveys or other established statistics at hand. Furthermore, an elaborate prediction has the potential to even be more accurate or more complete than a national survey. The model establishes a baseline metric for a country and builds in-depth understanding of countries\u2019 level of Internet connectivity. The geospatial analysis integrated in the modelling also reveals regional differences and local peculiarities. Ultimately, this work contributes to the field of offline population research and provides what is, to our knowledge, the first bespoke predictive model in this field. We have gained insights on the features and models that are most efficient in predicting Internet connectivity, so that national decision makers can fine-tune their strategies and own analysis while researchers and organizations can continue building on these initial findings. All processes, analysis and findings are duly documented and available as open data [1]. The collection of scripts and documentation can be cloned via the GitHub repository (https://github.com/DSSGxUK/itu). [1] National datasets are not part of the publicly available data package. National decision makers can nevertheless choose to also release their datasets and thus contribute to the replicability of the model and the global research community\u2019s enhanced understanding of the topic.","title":"Introduction"},{"location":"intro/#introduction","text":"","title":"Introduction"},{"location":"intro/#motivation","text":"Today, just over half of all people worldwide use the Internet. Yet in our fast-paced digital world, being offline excludes people from a host of opportunities. The Internet has grown beyond the remit of mere communication service and now grants access to global knowledge, social empowerment and new economic activities. In a word, the Internet fuels social and economic development and thus has been moving higher on national policy agendas around the world, all the more in the aftermath of the COVID-19 pandemic. Moreover, many countries lack data on Internet use in the society and can therefore not tackle the challenges most efficiently. Giga, the joint initiative by the International Telecommunication Union (ITU) and UNICEF, has set the goal to connect every school in the world to the Internet by 2030. Giga aims at giving children and youth the opportunity to get online and tap the potential of the Internet to their benefit. Connected, digitally savvy and enlightened youth can lead economies and societies towards a more productive, fulfilling, equal and sustainable future. What\u2019s more, as schools often serve as community hubs, the worth of connectivity can be extended to larger groups of the population. Arguably, connecting schools in no specific order does not allow for allocating resources where they are most needed and connecting communities that are most in need in a timely fashion. Giving priority to connecting schools that are closer to nodes or pro-actively apply for a connection may not serve national policy priorities best and could potentially even deepen gaps between urban and rural areas or across demographic groups, since the offline population is notoriously hard to target. This project therefore aims to provide national decision makers, multi-lateral agencies and development partners with a sound rationale for prioritization of schools to be connected, building on a large range of complementary datasets, advanced modelling and machine learning to identify the largest offline communities and craft connectivity roadmaps based on the most recent, relevant and accurate evidence available.","title":"Motivation"},{"location":"intro/#project-objectives","text":"This pioneering project aims at developing mechanisms for predicting the world's offline population, using school locations as the starting point. A multi-disciplinary team of fellows hosted by the University of Warwick jointly with experts from ITU and UNICEF have examined three case studies, namely Brazil, Thailand and the Philippines. Based on Big Data and machine learning techniques, the team has created a toolbox that can be used by researchers and institutions to investigate the state of connectivity within a country and enable near real-time national and regional benchmarking. The project outcome can be readily reproduced for other countries and the individual models can be enhanced for specific purposes. The evidence generated by the models can support digital policy planning and implementation at the national level and inform the work of all national stakeholders and development partners involved in connecting the unconnected at the sub-national, national and regional levels. Concretely, the ambition of this project has been three-fold. First, to estimate the share of households/individuals with Internet connectivity around a school. Internet connectivity is defined here as any ability to get online, either through a fixed or mobile broadband connection. This also means that individuals must own or have access to broadband-enabled devices. Based on population data for the area around each school, the offline population across communities is estimated. A rank of priority can then be given to communities and a recommended order in which schools can be connected in view of optimal resource allocation and fast-tracking the achievement of social and economic policy objectives. In addition, quick wins can be identified through further modelling and analysis, also taking into account data on computer availability and electrification in schools. Secondly, based on the best estimates of community level connectivity, to aggregate Internet connectivity data up to the country level. While such aggregate figures might already exist for some countries, this bottom-up approach will be helpful in countries where there are currently no census or surveys or other established statistics at hand. Furthermore, an elaborate prediction has the potential to even be more accurate or more complete than a national survey. The model establishes a baseline metric for a country and builds in-depth understanding of countries\u2019 level of Internet connectivity. The geospatial analysis integrated in the modelling also reveals regional differences and local peculiarities. Ultimately, this work contributes to the field of offline population research and provides what is, to our knowledge, the first bespoke predictive model in this field. We have gained insights on the features and models that are most efficient in predicting Internet connectivity, so that national decision makers can fine-tune their strategies and own analysis while researchers and organizations can continue building on these initial findings. All processes, analysis and findings are duly documented and available as open data [1]. The collection of scripts and documentation can be cloned via the GitHub repository (https://github.com/DSSGxUK/itu). [1] National datasets are not part of the publicly available data package. National decision makers can nevertheless choose to also release their datasets and thus contribute to the replicability of the model and the global research community\u2019s enhanced understanding of the topic.","title":"Project objectives"},{"location":"modapp/","text":"Model Application Thailand Our next big step was applying the best model to Thailand data. We were curious to apply the model as we were not sure that the same assumptions that are true for Brazil would hold true for Thailand. While the satellite data and vegetation may look the same, the national level economic and political indicators were not accounted for in the model. This is because, due to the project scope and capacity, we did not train multiple different national models. Had we had more time and data, perhaps this would have been an alternative route and we could have included some of this information. Instead, we trained a model exclusively on Brazil. For more discussion on future multi-national models, please see the conclusion. Therefore, the limitations for our model are rooted in basic assumptions that local areas can be comparable. A second limiting factor was the nature of Thailand data. We wanted to predict and evaluate the Thai schools in the same manner that we did for the Brazil schools. However, the survey data that served as ground truth for Brazil was on an enumeration area level while the survey data for Thailand was on a province area level (of which there are 77 in Thailand). These area units are not comparable and made the evaluation for Thailand more complicated. Furthermore, due to their small size, enumeration areas can be assumed to be more homogenous regarding demographics and also internet connectivity. As Thai provinces are much larger it seems to be unreasonable that one level of connectivity holds true for the entire province. Below you can see the school area level predictions, which mostly appear to be on a reasonable level, though the absence of fine-grained ground truth data prevents model evaluation. For that reason we subsequently perform the only evaluating measures that the ground truth data allows. First, we scale the school predictions up to a province level by calculating a population-weighted province connectivity share. We then compare these province-level values to the ground truth data visually and in a distribution plot. The large discrepancy between prediction and ground truth can potentially reflect upon our model and its questionable performance, but it also largely reflects on the raw survey data itself as we are skeptical of the amount of provinces that have 100% internet connectivity. Steps in our model application to new data. Please click here for a complete predict.py script. Click here for a Jupyter notebook with the XGBoost Predictions and its html equivalent. Using the predict_config, we load the Thailand data with the school points and the same predictors used by the original model. We then load the model from the provided model folder. The following code reloads the model and utilizes it to predict the connectivity on the Thailand dataset: After that, we examine the predictions on a map: Here are the maps that show the schools' predictions of relative online population from 0-1 in Thailand. Schools provided by OpenStreetMaps in this case. Subsequently, we modify the map to only display schools predicted to be below 50% internet connectivity, by the best Random Forest model and XGBoost model. Both models predicted 97 schools where less than half of the individuals around it are connected, but the patterns of schools differ slightly. From a first visual inspection we could draw the conclusion, that high offline school areas are mostly predicted in more northern areas and areas close to the Thailand national borders. In order to compare our predictions to the ground truth, we aggregated the schools up to a province level as survey data was only provided on that level. This measure of evaluation proved challenging for a number of reasons as stated above. The following graphics compare predicted and survey data province level connectivity shares on a country map and in a distribution histogram: By visual inspection, we can see that the model predictions on a province level diverge greatly from the existing ground truth. While the predictions are roughly normally distributed across the provinces with a small range of predictions, the range of ground truth connectivity shares appear to be much broader. Therefore, we are uncertain about the ability for our Brazil model to accurately predict school areas' internet connectivity in Thailand. Nevertheless, it seems unreasonable that more than half of Thai provinces have a 100% connectivity rate which raises the uncertainty, whether the large average error of 0.35 was caused by the model or the ground truth data. Philippines Model Application In a next step we applied the model trained on Brazilian data to the Philippines. As opposed to Thailand, survey microdata in this case was available on a significantly more granular level. The microdata contained geographic information down to the barangay level. The approximately 42000 Barangays are the lowest administrative districts in the Philippines and can more or less be defined as neighborhoods, quarters or small villages. Our dataset on which the model was applied consisted of around 4000 schools, that were located in Barangays featured in the survey. Furthermore, for the evaluation of model application and the subsequent model training (see below), we used individual instead of household microdata, since the household dataset showed insufficient sample sizes within a barangay and (therefore) too low variation. The following graphic shows the predictions for connectivity around schools in the Philippines when applying the Brazilian model. When evaluating the prediction we can clearly see, that they largely diverge from the ground truth microdata. Nevertheless, at this point it is not clear whether the significant discrepancy stems from the model's inaccuracy or from limitations within the survey data. On the one hand, multiple online sources such as XY state that the average connection rate in the Philippines is around XY% and much closer to our prediction, than to the survey data. On the other hand, it may well be, that the two countries are simply too different from each other to be able to apply the Brazil model to the Philippines. Model Training The fine-grained microdata and availability of barangay shapefiles made it possible to train a new model for the Philippines on the same dataset, that we applied the Brazilian model to. Using the same hyperparameters as for the champion Brazil model (hyper param?) we train an XGBoost model on the Philippines data. As depicted in the graphic below, the new model performs accurately in predicting online population around the featured schools, as the average error is decreased to only XY. Graphic XYZ shows XYZ (need graphics from utku) Conclusion: Extension to other countries The previous extensions of our analyses to further countries have yielded multiple important insights. First of all, the application of one country-specific (Brazilian) model resulted in significatly larger errors, than in the original countries. It is unclear to what extent the great disparity between the countries or the problematic microdata lead to this inaccuracy. However, it is clear that microdata should be as fine-grained and representative as possible in order to enable reasonable model evaluation and possibly model training. When comparing the applications to Thailand and the Philippines we can see, that the difference in data quality (and granularity) also resulted in diverging model evaluation and retraining possibilites. The model training on a new country turned out to be a great opportunity. At this point, we are able to train a model that accurately predicts the online population around schools if reasonable microdata is available. In the future, this does not necessarily have to be survey data, but could also stem from other data sources. Nevertheless, it might still be possible that a model is created that can be applied to any country even without microdata at hand. Further research should now aim to combine training datasets (e.g., from Brazil and the Philippines) and train a multi-national model. To extend to applicability to a global level, microdata for as many countries as possible should be retrieved in order to train one large model. Eventually it could be, that one highly predictive model is created that can be applied to any desired country. Further elaboration on this extension can be found in the Discussion section. Further Application Configuration file For both the school priorization and the following aggregation, a specific congfiguration file (predict_config.yaml) was used. It contains case specific information like the data paths, predictor variable set, country name, country population, and steps on implementing the champion model in mlflow. Within this file, these characteristics can easily be updated. If, for instance, one trains a new model, the configuration file is where you can point to the location of the new model as opposed to in the corresponding notebooks or scripts. This simplifies model prediction and minimizes human error. School Priorization Building on our school area predictions and additional information such as population or potential internet connectivity, we create a prioritization list of schools. One can change this priorization based on various indicators like relative offline population or absolute offline population. Furthermore, if the data contains geographic information, the priorization list can be filtered by a specific federal state or to schools in rural areas. As a first step, the feature-engineered training dataset for the respective country (in the following example: Brazil) and the pickled champion model are loaded. The imported model is then applied to the country's data and predicts online population for the schools featured in the dataset. Within this dataset, where schools are the rows, we can merge in more information on absolute population data of that school and information around the school's internet and computer availability. While population data is necessary to calculate the absolute offline population around a school, additional school information such as internet availability or pupil count might not be at hand for some countries. Therefore, that type of data is optional and not required for initial priorization. The following code chunks load the specific additional datasets and merge them to the initial training set. To only keep the correct absolute population variable we first drop the pixel population values featured in the model. If school location data was pulled by OSM and does not contain further information the chunk should be commented out. Since our model was not restricted to predict only values between 0 and 1, (for example it predicted values above 1 and below 0) we first standardized predicted values to be between these boundaries. Secondly, we multiply the prediction of online population share by the population count, which yields the estimated absolute online population around a school. The corresponding offline population is calculated by taking 1 - \"online population share\" and the multiplying this value with the population count. At this point, we've finished with the basic arithmetical steps and can create our custom priorization list. In this example, we first export the list of schools by absolute offline population, from greatest to smallest. In theory, connecting the first school in this list would potentially benefit the largest number of individuals in the school sample. We export additional information provided in the school data such as the school's name to facilitate use of the list. As our data for Brazil contains additional information (e.g. computer and internet availability), we not only add these variables to our exported file, but we also subset our schools to prioritize using these. It makes sense to exclude schools that, according to the UNICEF school data, already have internet access. Therefore, the second file lists only the offline schools ranked by the absolute offline population around it. The third subsetting step was to exclude the 10th population number decile of the sample, i.e. the outlying 10% of schools, that have the largest number of population around them. This rather exploratory step was done to investigate whether some schools are highly prioritized solely due to their high population numbers. Excluding the highest populated areas (in most cases large metropolitan areas) can lead towards the potentially less obvious hubs of offline population. For each of the priorization lists, we've create a choropleth map of schools and offline population. Country-level Aggregation A further step of model application was the aggregation to the country level. However, estimating the average of online population is not possible in this case, since it would weight each school area equally. A school area with 250 inhabitants would contribute as much to the national average a school area with 2500 people. Therefore, the national average is calculated slightly different. We sum up the previously calculated absolute online population and sum up the total population in our sample. If our enumeration area sample is representative for the whole country, dividing the total online population by the total population (both in sample) would yield a representative national level connectivity share. Ultimately, this proportion can be multiplied with the national population in order to get the absolute number of people connected to the internet. As a robustness check, the same calculations are then conducted using the ground truth connectivity data. If the required data is available, the aggregation can of course also be done to a province, federal state or other geographical level. The following table contains the Brazilian relative and absolute online population according to the aggregated prediction, aggregated ground truth and a third online source. The prediction comes close to the other two values, however it slightly overestimates the Brazil online population. Population data remarks Generally, some remarks regarding the absolute population data should be considered by users. For the priorization and the country level aggregation, the absolute number has to be treated cautiously due to school area overlap. In this example priorization list, we see that the first 6 schools have the same ground truth connectivity level (\"offline_g\"). The list indicates that these schools are all located in the same enumeration area (you can also tell from the geographic coordinates of the school location). The point from this list is that the radius around each school is most likely going to overlap with another school in an urban area and therefore individuals within this overlap will be counted more than once in the population numbers. While the number of roughly 50,000 people potentially reached with connecting the one specific school is accurate, we must bear in mind, that connecting 5 schools each with an absolute population of 50,000 people will not result in connecting 250,000 people to the internet because of the population overlap issue. Additionally, once the first school is connected to the internet, the priorization list will be altered since some individuals will already obtain internet through the first school leaving other schools on the list as a lower priority for connection (since prioritization is based exclusively on absolute population assisted). Due to the overlap of school areas, adding up the population numbers of the enumeration areas will always overestimate the total number of indiviudals featured in our analysis, as many will be counted more than once. It therefore prevents understanding an exact amount of individuals that can be connected to the internet on any geographic level. If researchers and organizations are aware of this and treat the population data cautiously, the list is a great resource. Even though densely populated areas like Sao Paolo will have school area (and therefore population data) overlap, the model can detect this enumeration area/neighborhood as a zone that would benefit from being connected. Providing one school with internet access in a densely populated area will in any case most likely not suffice in providing thousands of people with internet. Thus, it is a reasonable suggestion to connect multiple schools in the same area to the internet, even though the absolute population that is connected is somewhat skewed. Ultimately, it might also be worth further regard to reaggregate the predictions to the enumeration area level and treat those as one unit of interest. In reality, it might be a feasible approach to aim for connecting every school in an enumeration area (that systematically lacks internet) to the internet.","title":"Model Application"},{"location":"modapp/#model-application","text":"","title":"Model Application"},{"location":"modapp/#thailand","text":"Our next big step was applying the best model to Thailand data. We were curious to apply the model as we were not sure that the same assumptions that are true for Brazil would hold true for Thailand. While the satellite data and vegetation may look the same, the national level economic and political indicators were not accounted for in the model. This is because, due to the project scope and capacity, we did not train multiple different national models. Had we had more time and data, perhaps this would have been an alternative route and we could have included some of this information. Instead, we trained a model exclusively on Brazil. For more discussion on future multi-national models, please see the conclusion. Therefore, the limitations for our model are rooted in basic assumptions that local areas can be comparable. A second limiting factor was the nature of Thailand data. We wanted to predict and evaluate the Thai schools in the same manner that we did for the Brazil schools. However, the survey data that served as ground truth for Brazil was on an enumeration area level while the survey data for Thailand was on a province area level (of which there are 77 in Thailand). These area units are not comparable and made the evaluation for Thailand more complicated. Furthermore, due to their small size, enumeration areas can be assumed to be more homogenous regarding demographics and also internet connectivity. As Thai provinces are much larger it seems to be unreasonable that one level of connectivity holds true for the entire province. Below you can see the school area level predictions, which mostly appear to be on a reasonable level, though the absence of fine-grained ground truth data prevents model evaluation. For that reason we subsequently perform the only evaluating measures that the ground truth data allows. First, we scale the school predictions up to a province level by calculating a population-weighted province connectivity share. We then compare these province-level values to the ground truth data visually and in a distribution plot. The large discrepancy between prediction and ground truth can potentially reflect upon our model and its questionable performance, but it also largely reflects on the raw survey data itself as we are skeptical of the amount of provinces that have 100% internet connectivity. Steps in our model application to new data. Please click here for a complete predict.py script. Click here for a Jupyter notebook with the XGBoost Predictions and its html equivalent. Using the predict_config, we load the Thailand data with the school points and the same predictors used by the original model. We then load the model from the provided model folder. The following code reloads the model and utilizes it to predict the connectivity on the Thailand dataset: After that, we examine the predictions on a map: Here are the maps that show the schools' predictions of relative online population from 0-1 in Thailand. Schools provided by OpenStreetMaps in this case. Subsequently, we modify the map to only display schools predicted to be below 50% internet connectivity, by the best Random Forest model and XGBoost model. Both models predicted 97 schools where less than half of the individuals around it are connected, but the patterns of schools differ slightly. From a first visual inspection we could draw the conclusion, that high offline school areas are mostly predicted in more northern areas and areas close to the Thailand national borders. In order to compare our predictions to the ground truth, we aggregated the schools up to a province level as survey data was only provided on that level. This measure of evaluation proved challenging for a number of reasons as stated above. The following graphics compare predicted and survey data province level connectivity shares on a country map and in a distribution histogram: By visual inspection, we can see that the model predictions on a province level diverge greatly from the existing ground truth. While the predictions are roughly normally distributed across the provinces with a small range of predictions, the range of ground truth connectivity shares appear to be much broader. Therefore, we are uncertain about the ability for our Brazil model to accurately predict school areas' internet connectivity in Thailand. Nevertheless, it seems unreasonable that more than half of Thai provinces have a 100% connectivity rate which raises the uncertainty, whether the large average error of 0.35 was caused by the model or the ground truth data.","title":"Thailand"},{"location":"modapp/#philippines","text":"","title":"Philippines"},{"location":"modapp/#model-application_1","text":"In a next step we applied the model trained on Brazilian data to the Philippines. As opposed to Thailand, survey microdata in this case was available on a significantly more granular level. The microdata contained geographic information down to the barangay level. The approximately 42000 Barangays are the lowest administrative districts in the Philippines and can more or less be defined as neighborhoods, quarters or small villages. Our dataset on which the model was applied consisted of around 4000 schools, that were located in Barangays featured in the survey. Furthermore, for the evaluation of model application and the subsequent model training (see below), we used individual instead of household microdata, since the household dataset showed insufficient sample sizes within a barangay and (therefore) too low variation. The following graphic shows the predictions for connectivity around schools in the Philippines when applying the Brazilian model. When evaluating the prediction we can clearly see, that they largely diverge from the ground truth microdata. Nevertheless, at this point it is not clear whether the significant discrepancy stems from the model's inaccuracy or from limitations within the survey data. On the one hand, multiple online sources such as XY state that the average connection rate in the Philippines is around XY% and much closer to our prediction, than to the survey data. On the other hand, it may well be, that the two countries are simply too different from each other to be able to apply the Brazil model to the Philippines.","title":"Model Application"},{"location":"modapp/#model-training","text":"The fine-grained microdata and availability of barangay shapefiles made it possible to train a new model for the Philippines on the same dataset, that we applied the Brazilian model to. Using the same hyperparameters as for the champion Brazil model (hyper param?) we train an XGBoost model on the Philippines data. As depicted in the graphic below, the new model performs accurately in predicting online population around the featured schools, as the average error is decreased to only XY. Graphic XYZ shows XYZ (need graphics from utku)","title":"Model Training"},{"location":"modapp/#conclusion-extension-to-other-countries","text":"The previous extensions of our analyses to further countries have yielded multiple important insights. First of all, the application of one country-specific (Brazilian) model resulted in significatly larger errors, than in the original countries. It is unclear to what extent the great disparity between the countries or the problematic microdata lead to this inaccuracy. However, it is clear that microdata should be as fine-grained and representative as possible in order to enable reasonable model evaluation and possibly model training. When comparing the applications to Thailand and the Philippines we can see, that the difference in data quality (and granularity) also resulted in diverging model evaluation and retraining possibilites. The model training on a new country turned out to be a great opportunity. At this point, we are able to train a model that accurately predicts the online population around schools if reasonable microdata is available. In the future, this does not necessarily have to be survey data, but could also stem from other data sources. Nevertheless, it might still be possible that a model is created that can be applied to any country even without microdata at hand. Further research should now aim to combine training datasets (e.g., from Brazil and the Philippines) and train a multi-national model. To extend to applicability to a global level, microdata for as many countries as possible should be retrieved in order to train one large model. Eventually it could be, that one highly predictive model is created that can be applied to any desired country. Further elaboration on this extension can be found in the Discussion section.","title":"Conclusion: Extension to other countries"},{"location":"modapp/#further-application","text":"","title":"Further Application"},{"location":"modapp/#configuration-file","text":"For both the school priorization and the following aggregation, a specific congfiguration file (predict_config.yaml) was used. It contains case specific information like the data paths, predictor variable set, country name, country population, and steps on implementing the champion model in mlflow. Within this file, these characteristics can easily be updated. If, for instance, one trains a new model, the configuration file is where you can point to the location of the new model as opposed to in the corresponding notebooks or scripts. This simplifies model prediction and minimizes human error.","title":"Configuration file"},{"location":"modapp/#school-priorization","text":"Building on our school area predictions and additional information such as population or potential internet connectivity, we create a prioritization list of schools. One can change this priorization based on various indicators like relative offline population or absolute offline population. Furthermore, if the data contains geographic information, the priorization list can be filtered by a specific federal state or to schools in rural areas. As a first step, the feature-engineered training dataset for the respective country (in the following example: Brazil) and the pickled champion model are loaded. The imported model is then applied to the country's data and predicts online population for the schools featured in the dataset. Within this dataset, where schools are the rows, we can merge in more information on absolute population data of that school and information around the school's internet and computer availability. While population data is necessary to calculate the absolute offline population around a school, additional school information such as internet availability or pupil count might not be at hand for some countries. Therefore, that type of data is optional and not required for initial priorization. The following code chunks load the specific additional datasets and merge them to the initial training set. To only keep the correct absolute population variable we first drop the pixel population values featured in the model. If school location data was pulled by OSM and does not contain further information the chunk should be commented out. Since our model was not restricted to predict only values between 0 and 1, (for example it predicted values above 1 and below 0) we first standardized predicted values to be between these boundaries. Secondly, we multiply the prediction of online population share by the population count, which yields the estimated absolute online population around a school. The corresponding offline population is calculated by taking 1 - \"online population share\" and the multiplying this value with the population count. At this point, we've finished with the basic arithmetical steps and can create our custom priorization list. In this example, we first export the list of schools by absolute offline population, from greatest to smallest. In theory, connecting the first school in this list would potentially benefit the largest number of individuals in the school sample. We export additional information provided in the school data such as the school's name to facilitate use of the list. As our data for Brazil contains additional information (e.g. computer and internet availability), we not only add these variables to our exported file, but we also subset our schools to prioritize using these. It makes sense to exclude schools that, according to the UNICEF school data, already have internet access. Therefore, the second file lists only the offline schools ranked by the absolute offline population around it. The third subsetting step was to exclude the 10th population number decile of the sample, i.e. the outlying 10% of schools, that have the largest number of population around them. This rather exploratory step was done to investigate whether some schools are highly prioritized solely due to their high population numbers. Excluding the highest populated areas (in most cases large metropolitan areas) can lead towards the potentially less obvious hubs of offline population. For each of the priorization lists, we've create a choropleth map of schools and offline population.","title":"School Priorization"},{"location":"modapp/#country-level-aggregation","text":"A further step of model application was the aggregation to the country level. However, estimating the average of online population is not possible in this case, since it would weight each school area equally. A school area with 250 inhabitants would contribute as much to the national average a school area with 2500 people. Therefore, the national average is calculated slightly different. We sum up the previously calculated absolute online population and sum up the total population in our sample. If our enumeration area sample is representative for the whole country, dividing the total online population by the total population (both in sample) would yield a representative national level connectivity share. Ultimately, this proportion can be multiplied with the national population in order to get the absolute number of people connected to the internet. As a robustness check, the same calculations are then conducted using the ground truth connectivity data. If the required data is available, the aggregation can of course also be done to a province, federal state or other geographical level. The following table contains the Brazilian relative and absolute online population according to the aggregated prediction, aggregated ground truth and a third online source. The prediction comes close to the other two values, however it slightly overestimates the Brazil online population.","title":"Country-level Aggregation"},{"location":"modapp/#population-data-remarks","text":"Generally, some remarks regarding the absolute population data should be considered by users. For the priorization and the country level aggregation, the absolute number has to be treated cautiously due to school area overlap. In this example priorization list, we see that the first 6 schools have the same ground truth connectivity level (\"offline_g\"). The list indicates that these schools are all located in the same enumeration area (you can also tell from the geographic coordinates of the school location). The point from this list is that the radius around each school is most likely going to overlap with another school in an urban area and therefore individuals within this overlap will be counted more than once in the population numbers. While the number of roughly 50,000 people potentially reached with connecting the one specific school is accurate, we must bear in mind, that connecting 5 schools each with an absolute population of 50,000 people will not result in connecting 250,000 people to the internet because of the population overlap issue. Additionally, once the first school is connected to the internet, the priorization list will be altered since some individuals will already obtain internet through the first school leaving other schools on the list as a lower priority for connection (since prioritization is based exclusively on absolute population assisted). Due to the overlap of school areas, adding up the population numbers of the enumeration areas will always overestimate the total number of indiviudals featured in our analysis, as many will be counted more than once. It therefore prevents understanding an exact amount of individuals that can be connected to the internet on any geographic level. If researchers and organizations are aware of this and treat the population data cautiously, the list is a great resource. Even though densely populated areas like Sao Paolo will have school area (and therefore population data) overlap, the model can detect this enumeration area/neighborhood as a zone that would benefit from being connected. Providing one school with internet access in a densely populated area will in any case most likely not suffice in providing thousands of people with internet. Thus, it is a reasonable suggestion to connect multiple schools in the same area to the internet, even though the absolute population that is connected is somewhat skewed. Ultimately, it might also be worth further regard to reaggregate the predictions to the enumeration area level and treat those as one unit of interest. In reality, it might be a feasible approach to aim for connecting every school in an enumeration area (that systematically lacks internet) to the internet.","title":"Population data remarks"},{"location":"model/","text":"Modeling Section: The model we needed to train was a regression model that predicts the average share of households' internet connectivity in an enumeration area on a scale from 0 to 1. A prediction of 0 means that of the households surveyed, no households in the enumeration area stated that they had access to internet, whereas a prediction of 1 means that every household surveyed in the enumeration area had access to internet. In the Brazilian survey data, the average responding households per enumeration area were around 11. Logically, most of the responses fell between 0 and 1, indicating that some but not all families had internet access. Later on, we experimented with transforming the approach into a classification problem, however it did not increase model accuracy. Training Set EDA Once we created the training dataset was (for more information, see data gathering), we performed some Exploratory Data Analysis on it. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file. Mlflow Set-up (Optional) In order to track our models, we set up autologging in mlflow. Mlflow is an exciting tool for logging machine learning models, their respective KPIs and additional information. We set up our model training so that the python scripts create a new experiment for each run that logs each of the model parameters when we did hyperparameter tuning and then logs the best parameter at the top. In this way, we were able to compare the various parameters logged in each run to determine how to change the grid space of the hyperparameters. We also were then able to compare the different models to each other. Additionaly, we logged the predictors, requirements for packages and dependencies for each run. Every run's winning model was logged as an artifact, so one can easily reload the model and apply it to other data. In order to make our following analyses as reproducible as possible, we are providing a few of our winning models from different model classes. Below you can see a screenshot of mlflow which logs the best runs, with the best hyperparameters and a custom metric for evaluation. On the side, you can also see the list of other experiments we ran with different model classes. ``` #### mlflow setup #### # save runs mlflow.set_tracking_uri(\"file:///files/mlruns\") mlflow.tracking.get_tracking_uri() # Naming the set_experiment dt = date.today().strftime('%d/%m/%Y') experiment_name = dt + model_config['meta']['experiment_name'] mlflow.set_experiment(experiment_name) mlflow_client = mlflow.tracking.MlflowClient() experiment_id = mlflow_client.get_experiment_by_name(experiment_name).experiment_id ``` Here you can see the simplicity of reloading the model artifact later on and applying it to new data: Model Configuration Once we have mlflow and the respective model_config.yaml file set up, we can run many different experiments using our .py scripts by simply changing a some things within the configuration yaml file. Click here to see the full yaml file. Below you can also see how it was set up. We use this to steer our scripts and set our parameters. We set up the input data at the top which is the training data, then label the target and predictor variables as well as the name of the experiment and a brief description in run_name. Under the parameters section, we set parameters like test size (which is crucial), the amount of cross validation folds to do, the number of iterations and the threshold for our custom metric. The threshold tells the model which percent of schools with low internet connectivity to focus on. Then within parameters, there are different sections based on what type of model you might decide to run. Our .yaml file contains parameters for grid search within Random Forest, LightGBM and XGBoost. In our current scripts, we've commented out the mlflow logging for hyperparameter tuning. Model Training We tried our 7 different model classes and ran more than 100 experiments each containing 20 runs at minimum that tried various parameters in order to determine which model had the best accuracy. We experimented with different combinations of parameters and predictors. Below is the final list of predictors we used and a heat map displaying their collinearity. As you can see, we do not find high multi-collinearity among our predictors except with the mean global human modification and the mean average radiance. However, both predictors showed high feature importances and were therefore kept in the model. In this figure, one can see the correlation between predictors and our target variable. Predictors like the global human modification and average radiance showed the largest positive correlations. Another way to improve accuracy was by building a custom metric in order to score both our test set within our cross validation and our final holdout set. The metric calculates errors specifically by taking all predictions below .3 (or another threshold, we also experimented with .5) subtracting that from the ground truth below .3 (or another threshold), taking the absolute value and then returning the average of all those errors. Below please find a code snippet of our custom metric. #Create custom scoring def custom_eval_metric(y_true, y_pred): errors_low = abs(y_pred[y_pred<0.3] - np.asarray(y_true[y_pred<0.3]).flatten()) return np.mean(errors_low) custom_scorer = make_scorer(custom_eval_metric, greater_is_better = False) # define grid search search = GridSearchCV(model, parameters, scoring = custom_scorer, cv = inner_cv , refit=True, verbose = 2) We built the metric as we wanted to put the focus on having better accuracy on schools with lower internet connectivity. Before instituting the custom metric, our models performed well with predicting the medium values, but they had much larger errors at either end of the spectrum and particularly on the low values. In order to remedy this issue, we first dropped any rows that had an internet connectivity of zero (there were 23 of them). We dropped the zeros because they were most likely occuring due to incomplete data rather than enumeration areas truly having no households connected. Furthermore, as a value of 0 only occured for 23 observations, it did not severely impact the data class balancing. Secondly, we instituted our custom metric which trained the model to minimize the error score under the .3 level of prediction. Within our scripts, we offer two ways of doing cross validation. One is by grid search which searches through every combination of the hyperparameter grid space to find the best combination. The other format is Randomized Grid Search which searches through a random combination and is steered by the number of iterations (or combinations to test out) given to it in the config file. As we have tested all the hyperparameters, the current grid space is much smaller than prior and we have chosen to include gridsearch cv with randomized cv commented out in case further research wants to add more parameters and repeat the tuning. The resulting champion out of over 2000 models was an XGBoost model with an average error of .06. Specifically for under the .3 threshold, the model had an average error of .05. This means that the overall and low-threshold accuracy appears to be on an adequate level, as on average the predictions are off by just 5 percentage points from the ground truth value. Below, you can see the list of all model classes we tried. Feel free to try out running these models yourselves or reading the code by clicking on the hyperlinked script. There is further documentation within each script on how it runs, and how it works with mlflow logging. Linear Regression Python script with Mlflow Random Forest HTML File Jupyter Notebook Python Script with Mlflow Python script without Mlflow XGBoost HTML File Jupyter Notebook #this is not correct Python Script with Mlflow Python script without Mlflow LightGBM HTML File Jupyter Notebook Python Script with Mlflow SVM Python Script with Mlflow Neural Net Python Script with Mlflow Random Forest Classifier HTML File Jupyter Notebook Model Evaluation and Results Below we see a comparison of the average low connectivity errors of all models. Clearly the Random Forest and XG Boost model were most accurate in predicting the low connectivity school areas. Click on this link to see a notebook with the model comparisons. Click here for the HTML version. The winning XGboost model produced an error of .06 and a low average error of .05 with the hyper parameters of: eta: .2, max_depth: 9 and n_estimators: 550. Click on this link for the notebook with the Random Forest Predictions and click on this link for the notebook with the XGBoost Predictions . Here is a map of our predictions for schools within Brazil. Figure 1 displays the location for all the schools where the ground truth is less than 30% connected to the internet. There are 69 schools in Brazil survey data that have less than 30% internet connectivity. Figure 2 shows schools where our Random Forest model prediction was actually less than 30% connected to the internet. While we can see that not all schools below 30% were predicted correctly below the threshold, the low error score indicates that if the model succeeds below-threshold prediction, it performs well. While the Random Forest model predicted 14 schools under 30%, the XGBoost predicted 29. Figure 3 shows the predictions for all schools in the test set mapped out. This gives us an understanding of where the higher and lower connected school areas are located regionally. It appears that the higher connected schools areas are on the coast (the yellows and light greens) while the lower connected schools are located more inland. In Figure 4, we see the errors mapped out for the schools in the test set. For most schools the error scores appear to be on a low level, however large error predictions should be further examined, especially if they occur systematically at schools with low average connectivity. This graph compares predictions to reality in a scatter plot. We can see that the points are mostly close to the line except within the lower range of connectivity. Then we examine the residuals compared to reality. Most residuals hug tightly to the line except for some observations at the very low and high end. Lastly, we also see the comparison of distributions between reality and predictions. While the predictions curve is significantly steeper, the overall curves generally follow each other and have a similar center. Model Interpretation As part of our winning models, we wanted to see which predictors had high feature importances within the model. Below, is the graph for both the winning Random Forest and XGBoost model feature importances. For both models, the average radiance seems to have significant predictive power, however the importances of other features differ largely between RF and XGBoost. While average download speed has a large feature importance value for XGBoost, it was one of the least important features in the RF model. Moreover, Vegetation Index and Facebook users are key features in the RF model but only played a marginal role in XGBoost. !! wrong variable names and potentially wrong graphicc Subsequently, we further investigated the effects of features on the prediction by examining the Shapley values. The Shapely value is an indicator that originally stems from Game Theory but is commonly used in Machine Learning to determine the contribution of a feature to a prediction. The graphic below shows the scattered effects that predictors had on one specific prediction for the champion XGBoost model. In addition, it indicates how relatively high or low values of features impacted the overall prediction. While low impact features like average download and upload speed look normally distributed around zero, important features like Facebook can be interpreted more reasonably. For this model, low monthly Facebook users resulted in a (much) lower prediction of online population. Similarly, low values of average radiance yielded lower predictions. In order to examine impact of the features in detail, we looked at Shapely values where the model performed particularly well and particularly poorly (i.e. very low and high errors). For errors larger than 0.4 most of the shapley values are scattered around 0, however from the image below we can not make out a clear pattern around why these predictions turned out to be poorly. Similarly, an inspection of Shapely values where the errors were lower than 0.05 does not yield an obvious difference to the overall Shapely values. Thus explaining the high performance of predictors for schools with error lower than 0.5. We evaluated how the feature importance changes across the range of feature values by creating line and scatter plots indicating the value und respective importance. This example depicts the feature importances for the range of average radiance values. We can observe a clear pattern, that the more the radiance deviates from the mean either in a positive or negative direction, the stronger the respective effect on the prediction appears to be. Very low values have strong negative effects, whereas large values typically have a strong positive effect on the prediction. Ultimately, we spot-checked a single school area prediction with observing how the collection of features influenced this particular prediction. This again can be done for all school areas but also subsetted for a set of observations with very high or low errors. In the example below, we are examining a school area prediction with a high error. When inspecting this particular prediction, we observe that Facebook data did not have a significant impact. This might be one reason the error is so high.","title":"Modeling"},{"location":"model/#modeling-section","text":"The model we needed to train was a regression model that predicts the average share of households' internet connectivity in an enumeration area on a scale from 0 to 1. A prediction of 0 means that of the households surveyed, no households in the enumeration area stated that they had access to internet, whereas a prediction of 1 means that every household surveyed in the enumeration area had access to internet. In the Brazilian survey data, the average responding households per enumeration area were around 11. Logically, most of the responses fell between 0 and 1, indicating that some but not all families had internet access. Later on, we experimented with transforming the approach into a classification problem, however it did not increase model accuracy.","title":"Modeling Section:"},{"location":"model/#training-set-eda","text":"Once we created the training dataset was (for more information, see data gathering), we performed some Exploratory Data Analysis on it. Click Here for the full notebook of explanatory visualizations. Click here for the Jupyter Notebook .ipynb file.","title":"Training Set EDA"},{"location":"model/#mlflow-set-up-optional","text":"In order to track our models, we set up autologging in mlflow. Mlflow is an exciting tool for logging machine learning models, their respective KPIs and additional information. We set up our model training so that the python scripts create a new experiment for each run that logs each of the model parameters when we did hyperparameter tuning and then logs the best parameter at the top. In this way, we were able to compare the various parameters logged in each run to determine how to change the grid space of the hyperparameters. We also were then able to compare the different models to each other. Additionaly, we logged the predictors, requirements for packages and dependencies for each run. Every run's winning model was logged as an artifact, so one can easily reload the model and apply it to other data. In order to make our following analyses as reproducible as possible, we are providing a few of our winning models from different model classes. Below you can see a screenshot of mlflow which logs the best runs, with the best hyperparameters and a custom metric for evaluation. On the side, you can also see the list of other experiments we ran with different model classes. ``` #### mlflow setup #### # save runs mlflow.set_tracking_uri(\"file:///files/mlruns\") mlflow.tracking.get_tracking_uri() # Naming the set_experiment dt = date.today().strftime('%d/%m/%Y') experiment_name = dt + model_config['meta']['experiment_name'] mlflow.set_experiment(experiment_name) mlflow_client = mlflow.tracking.MlflowClient() experiment_id = mlflow_client.get_experiment_by_name(experiment_name).experiment_id ``` Here you can see the simplicity of reloading the model artifact later on and applying it to new data:","title":"Mlflow Set-up (Optional)"},{"location":"model/#model-configuration","text":"Once we have mlflow and the respective model_config.yaml file set up, we can run many different experiments using our .py scripts by simply changing a some things within the configuration yaml file. Click here to see the full yaml file. Below you can also see how it was set up. We use this to steer our scripts and set our parameters. We set up the input data at the top which is the training data, then label the target and predictor variables as well as the name of the experiment and a brief description in run_name. Under the parameters section, we set parameters like test size (which is crucial), the amount of cross validation folds to do, the number of iterations and the threshold for our custom metric. The threshold tells the model which percent of schools with low internet connectivity to focus on. Then within parameters, there are different sections based on what type of model you might decide to run. Our .yaml file contains parameters for grid search within Random Forest, LightGBM and XGBoost. In our current scripts, we've commented out the mlflow logging for hyperparameter tuning.","title":"Model Configuration"},{"location":"model/#model-training","text":"We tried our 7 different model classes and ran more than 100 experiments each containing 20 runs at minimum that tried various parameters in order to determine which model had the best accuracy. We experimented with different combinations of parameters and predictors. Below is the final list of predictors we used and a heat map displaying their collinearity. As you can see, we do not find high multi-collinearity among our predictors except with the mean global human modification and the mean average radiance. However, both predictors showed high feature importances and were therefore kept in the model. In this figure, one can see the correlation between predictors and our target variable. Predictors like the global human modification and average radiance showed the largest positive correlations. Another way to improve accuracy was by building a custom metric in order to score both our test set within our cross validation and our final holdout set. The metric calculates errors specifically by taking all predictions below .3 (or another threshold, we also experimented with .5) subtracting that from the ground truth below .3 (or another threshold), taking the absolute value and then returning the average of all those errors. Below please find a code snippet of our custom metric. #Create custom scoring def custom_eval_metric(y_true, y_pred): errors_low = abs(y_pred[y_pred<0.3] - np.asarray(y_true[y_pred<0.3]).flatten()) return np.mean(errors_low) custom_scorer = make_scorer(custom_eval_metric, greater_is_better = False) # define grid search search = GridSearchCV(model, parameters, scoring = custom_scorer, cv = inner_cv , refit=True, verbose = 2) We built the metric as we wanted to put the focus on having better accuracy on schools with lower internet connectivity. Before instituting the custom metric, our models performed well with predicting the medium values, but they had much larger errors at either end of the spectrum and particularly on the low values. In order to remedy this issue, we first dropped any rows that had an internet connectivity of zero (there were 23 of them). We dropped the zeros because they were most likely occuring due to incomplete data rather than enumeration areas truly having no households connected. Furthermore, as a value of 0 only occured for 23 observations, it did not severely impact the data class balancing. Secondly, we instituted our custom metric which trained the model to minimize the error score under the .3 level of prediction. Within our scripts, we offer two ways of doing cross validation. One is by grid search which searches through every combination of the hyperparameter grid space to find the best combination. The other format is Randomized Grid Search which searches through a random combination and is steered by the number of iterations (or combinations to test out) given to it in the config file. As we have tested all the hyperparameters, the current grid space is much smaller than prior and we have chosen to include gridsearch cv with randomized cv commented out in case further research wants to add more parameters and repeat the tuning. The resulting champion out of over 2000 models was an XGBoost model with an average error of .06. Specifically for under the .3 threshold, the model had an average error of .05. This means that the overall and low-threshold accuracy appears to be on an adequate level, as on average the predictions are off by just 5 percentage points from the ground truth value. Below, you can see the list of all model classes we tried. Feel free to try out running these models yourselves or reading the code by clicking on the hyperlinked script. There is further documentation within each script on how it runs, and how it works with mlflow logging. Linear Regression Python script with Mlflow Random Forest HTML File Jupyter Notebook Python Script with Mlflow Python script without Mlflow XGBoost HTML File Jupyter Notebook #this is not correct Python Script with Mlflow Python script without Mlflow LightGBM HTML File Jupyter Notebook Python Script with Mlflow SVM Python Script with Mlflow Neural Net Python Script with Mlflow Random Forest Classifier HTML File Jupyter Notebook","title":"Model Training"},{"location":"model/#model-evaluation-and-results","text":"Below we see a comparison of the average low connectivity errors of all models. Clearly the Random Forest and XG Boost model were most accurate in predicting the low connectivity school areas. Click on this link to see a notebook with the model comparisons. Click here for the HTML version. The winning XGboost model produced an error of .06 and a low average error of .05 with the hyper parameters of: eta: .2, max_depth: 9 and n_estimators: 550. Click on this link for the notebook with the Random Forest Predictions and click on this link for the notebook with the XGBoost Predictions . Here is a map of our predictions for schools within Brazil. Figure 1 displays the location for all the schools where the ground truth is less than 30% connected to the internet. There are 69 schools in Brazil survey data that have less than 30% internet connectivity. Figure 2 shows schools where our Random Forest model prediction was actually less than 30% connected to the internet. While we can see that not all schools below 30% were predicted correctly below the threshold, the low error score indicates that if the model succeeds below-threshold prediction, it performs well. While the Random Forest model predicted 14 schools under 30%, the XGBoost predicted 29. Figure 3 shows the predictions for all schools in the test set mapped out. This gives us an understanding of where the higher and lower connected school areas are located regionally. It appears that the higher connected schools areas are on the coast (the yellows and light greens) while the lower connected schools are located more inland. In Figure 4, we see the errors mapped out for the schools in the test set. For most schools the error scores appear to be on a low level, however large error predictions should be further examined, especially if they occur systematically at schools with low average connectivity. This graph compares predictions to reality in a scatter plot. We can see that the points are mostly close to the line except within the lower range of connectivity. Then we examine the residuals compared to reality. Most residuals hug tightly to the line except for some observations at the very low and high end. Lastly, we also see the comparison of distributions between reality and predictions. While the predictions curve is significantly steeper, the overall curves generally follow each other and have a similar center.","title":"Model Evaluation and Results"},{"location":"model/#model-interpretation","text":"As part of our winning models, we wanted to see which predictors had high feature importances within the model. Below, is the graph for both the winning Random Forest and XGBoost model feature importances. For both models, the average radiance seems to have significant predictive power, however the importances of other features differ largely between RF and XGBoost. While average download speed has a large feature importance value for XGBoost, it was one of the least important features in the RF model. Moreover, Vegetation Index and Facebook users are key features in the RF model but only played a marginal role in XGBoost. !! wrong variable names and potentially wrong graphicc Subsequently, we further investigated the effects of features on the prediction by examining the Shapley values. The Shapely value is an indicator that originally stems from Game Theory but is commonly used in Machine Learning to determine the contribution of a feature to a prediction. The graphic below shows the scattered effects that predictors had on one specific prediction for the champion XGBoost model. In addition, it indicates how relatively high or low values of features impacted the overall prediction. While low impact features like average download and upload speed look normally distributed around zero, important features like Facebook can be interpreted more reasonably. For this model, low monthly Facebook users resulted in a (much) lower prediction of online population. Similarly, low values of average radiance yielded lower predictions. In order to examine impact of the features in detail, we looked at Shapely values where the model performed particularly well and particularly poorly (i.e. very low and high errors). For errors larger than 0.4 most of the shapley values are scattered around 0, however from the image below we can not make out a clear pattern around why these predictions turned out to be poorly. Similarly, an inspection of Shapely values where the errors were lower than 0.05 does not yield an obvious difference to the overall Shapely values. Thus explaining the high performance of predictors for schools with error lower than 0.5. We evaluated how the feature importance changes across the range of feature values by creating line and scatter plots indicating the value und respective importance. This example depicts the feature importances for the range of average radiance values. We can observe a clear pattern, that the more the radiance deviates from the mean either in a positive or negative direction, the stronger the respective effect on the prediction appears to be. Very low values have strong negative effects, whereas large values typically have a strong positive effect on the prediction. Ultimately, we spot-checked a single school area prediction with observing how the collection of features influenced this particular prediction. This again can be done for all school areas but also subsetted for a set of observations with very high or low errors. In the example below, we are examining a school area prediction with a high error. When inspecting this particular prediction, we observe that Facebook data did not have a significant impact. This might be one reason the error is so high.","title":"Model Interpretation"},{"location":"Images/Images/","text":"","title":"Images"}]}